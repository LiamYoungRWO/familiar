#' @include FamiliarS4Generics.R
#' @include FamiliarS4Classes.R
NULL

#'@title Internal function to create a familiarData object.
#'
#'@description Compute various data related to model performance and calibration
#'  from the provided dataset and `familiarEnsemble` object and store it as a
#'  `familiarData` object.
#'
#'@param object A `familiarEnsemble` object, which is an ensemble of one or more
#'  `familiarModel` objects.
#'@param data A `dataObject` object, `data.table` or `data.frame` that
#'  constitutes the data that are used to compute
#'@param is_pre_processed Flag that indicates whether the data was already
#'  pre-processed externally, e.g. normalised and clustered. Only used if the
#'  `data` argument is a `data.table` or `data.frame`.
#'@param cl Cluster created using the `parallel` package. This cluster is then
#'  used to speed up computation through parallelisation.
#'@param time_max Time point which is used as the benchmark for e.g. cumulative
#'  risks generated by random forest, or the cutoff for Uno's concordance index.
#'  If not provided explicitly, this parameter is read from settings used at
#'  creation of the underlying `familiarModel` objects. Only used for `survival`
#'  outcomes.
#'@param eval_times One or more time points that are used for assessing
#'  calibration in survival problems. This is done as expected and observed
#'  survival probabilities depend on time. If not provided explicitly, this
#'  parameter is read from settings used at creation of the underlying
#'  `familiarModel` objects. Only used for `survival` outcomes.
#'@param aggregation_method Method for aggregating variable importances for the
#'  purpose of evaluation. Variable importances are determined during feature
#'  selection steps and after training the model. Both types are evaluated, but
#'  feature selection variable importance is only evaluated at run-time.
#'
#'  See the documentation for the `vimp_aggregation_method` argument in
#'  `summon_familiar` for information concerning the different available
#'  methods.
#'
#'  If not provided explicitly, this parameter is read from settings used at
#'  creation of the underlying `familiarModel` objects.
#'@param rank_threshold The threshold used to  define the subset of highly
#'  important features during evaluation.
#'
#'  See the documentation for the `vimp_aggregation_rank_threshold` argument in
#'  `summon_familiar` for more information.
#'
#'  If not provided explicitly, this parameter is read from settings used at
#'  creation of the underlying `familiarModel` objects.
#'@param ensemble_method Method for ensembling predictions from models for the
#'  same sample. Available methods are:
#'
#'  * `mean`: Use the mean of the predicted values as the ensemble value for a
#'  sample.
#'@param risk_ensemble_method Method for ensembling the risk group assignments
#'  from different models for the same sample.
#'
#'  The following methods are available:
#'
#'  * `ensemble_mean`: Risk groups are determined for each sample using the
#'  threshold values of the different models in the ensemble. The risk groups
#'  are treated as ordinal and encoded using integer values. The mean of the
#'  encoded values is computed. The risk group bin containing the mean value is
#'  then used as the ensemble-based risk group.
#'
#'  * `ensemble_mode` (default): Risk groups are determined for each sample
#'  using the threshold values of the different models in the ensemble. The most
#'  commonly assigned risk group is then used as the ensemble-based risk group.
#'
#'  * `median_threshold`: The median threshold value for each risk group is
#'  determined from the threshold values of the different models in the
#'  ensemble. The resulting threshold(s) are then applied to the ensemble
#'  prediction of a sample to identify the ensemble-based risk group.
#'
#'  * `mean_threshold`: Similar to `median_threshold`, but uses the mean
#'  threshold value for each risk group.
#'
#'  If not provided explicitly, this parameter is read from settings used at
#'  creation of the underlying `familiarModel` objects. This parameter is only
#'  relevant for `survival` outcomes.
#'@param metric One or more metrics for assessing model performance. See the
#'  vignette on performance metrics for the available metrics. If not provided
#'  explicitly, this parameter is read from settings used at creation of the
#'  underlying `familiarModel` objects.
#'@param feature_cluster_method The method used to perform clustering. These are
#'  the same methods as for the `cluster_method` configuration parameter:
#'  `none`, `hclust`, `agnes`, `diana` and `pam`.
#'
#'  `none` cannot be used when extracting data regarding mutual correlation or
#'  feature expressions.
#'
#'  If not provided explicitly, this parameter is read from settings used at
#'  creation of the underlying `familiarModel` objects.
#'
#'@param feature_linkage_method The method used for agglomerative clustering in
#'  `hclust` and `agnes`. These are the same methods as for the
#'  `cluster_linkage_method` configuration parameter: `average`, `single`,
#'  `complete`, `weighted`, and `ward`.
#'
#'  If not provided explicitly, this parameter is read from settings used at
#'  creation of the underlying `familiarModel` objects.
#'
#'@param feature_cluster_cut_method The method used to divide features into
#'  separate clusters. The available methods are the same as for the
#'  `cluster_cut_method` configuration parameter: `silhouette`, `fixed_cut` and
#'  `dynamic_cut`.
#'
#'  `silhouette` is available for all cluster methods, but `fixed_cut` only
#'  applies to methods that create hierarchical trees (`hclust`, `agnes` and
#'  `diana`). `dynamic_cut` requires the `dynamicTreeCut` package and can only
#'  be used with `agnes` and `hclust`.
#'
#'  If not provided explicitly, this parameter is read from settings used at
#'  creation of the underlying `familiarModel` objects.
#'
#'@param feature_similarity_threshold The threshold level for pair-wise
#'  similarity that is required to form feature clusters with the `fixed_cut`
#'  method.
#'
#'  If not provided explicitly, this parameter is read from settings used at
#'  creation of the underlying `familiarModel` objects.
#'
#'@param feature_similarity_metric Metric to determine pairwise similarity
#'  between features. Similarity is computed in the same manner as for
#'  clustering, and `feature_similarity_metric` therefore has the same options
#'  as `cluster_similarity_metric`: `mcfadden_r2`, `cox_snell_r2`,
#'  `nagelkerke_r2`, `spearman`, `kendall` and `pearson`.
#'
#'  If not provided explicitly, this parameter is read from settings used at
#'  creation of the underlying `familiarModel` objects.
#'
#'@param sample_cluster_method The method used to perform clustering based on
#'  distance between samples. These are the same methods as for the
#'  `cluster_method` configuration parameter: `hclust`, `agnes`, `diana`
#'  and `pam`.
#'
#'  `none` cannot be used when extracting data for feature expressions.
#'
#'  If not provided explicitly, this parameter is read from settings used at
#'  creation of the underlying `familiarModel` objects.
#'
#'@param sample_linkage_method The method used for agglomerative clustering in
#'  `hclust` and `agnes`. These are the same methods as for the
#'  `cluster_linkage_method` configuration parameter: `average`, `single`,
#'  `complete`, `weighted`, and `ward`.
#'
#'  If not provided explicitly, this parameter is read from settings used at
#'  creation of the underlying `familiarModel` objects.
#'
#'@param sample_similarity_metric Metric to determine pairwise similarity
#'  between samples. Similarity is computed in the same manner as for
#'  clustering, but `sample_similarity_metric` has different options that are
#'  better suited to computing distance between samples instead of between
#'  features: `gower`, `euclidean`.
#'
#'  The underlying feature data is scaled to the \eqn{[0, 1]} range (for
#'  numerical features) using the feature values across the samples. The
#'  normalisation parameters required can optionally be computed from feature
#'  data with the outer 5% (on both sides) of feature values trimmed or
#'  winsorised. To do so append `_trim` (trimming) or `_winsor` (winsorising) to
#'  the metric name. This reduces the effect of outliers somewhat.
#'
#'  If not provided explicitly, this parameter is read from settings used at
#'  creation of the underlying `familiarModel` objects.
#'
#'@param metric_alpha Numeric value for the alpha level at which confidence (or
#'  credibility intervals) intervals are determined using bootstrap estimation.
#'  The number of bootstraps depend on `metric_alpha`. `familiar` uses the rule
#'  of thumb \eqn{n = 20 / \alpha} to determine the number of required
#'  bootstraps.
#'
#'  If not provided explicitly, this parameter is read from settings used at
#'  creation of the underlying `familiarModel` objects.
#'@param icc_type String indicating the type of intraclass correlation
#'  coefficient (`1`, `2` or `3`) that should be used to compute robustness for
#'  features in repeated measurements during the evaluation of univariate
#'  importance. These types correspond to the types in Shrout and Fleiss (1979).
#'  If not provided explicitly, this parameter is read from settings used at
#'  creation of the underlying `familiarModel` objects.
#'@param verbose Flag to indicate whether feedback should be provided on the
#'  computation and extraction of various data elements.
#'@param data_element String indicating which data elements are to be extracted.
#'  Default is `all`, but specific elements can be specified to speed up
#'  computations if not all elements are to be computed. This is an internal
#'  parameter that is set by, e.g. the `export_model_vimp` method.
#'@param ... Unused arguments.
#'
#'@return A `familiarData` object.
#'@references 1. Shrout, P. E. & Fleiss, J. L. Intraclass correlations: uses in
#'  assessing rater reliability. Psychol. Bull. 86, 420–428 (1979).
#'@md
#'@keywords internal
setGeneric("extract_data", function(object, data, is_pre_processed=FALSE, cl=NULL,
                                    time_max=waiver(),
                                    aggregation_method=waiver(),
                                    rank_threshold=waiver(),
                                    ensemble_method=waiver(),
                                    risk_ensemble_method=waiver(),
                                    eval_times=waiver(),
                                    metric=waiver(),
                                    feature_cluster_method=waiver(),
                                    feature_cluster_cut_method=waiver(),
                                    feature_linkage_method=waiver(),
                                    feature_similarity_metric=waiver(),
                                    feature_similarity_threshold=waiver(),
                                    sample_cluster_method=waiver(),
                                    sample_linkage_method=waiver(),
                                    sample_similarity_metric=waiver(),
                                    metric_alpha=waiver(),
                                    icc_type=waiver(),
                                    verbose=FALSE,
                                    data_element="all", ...) standardGeneric("extract_data"))

#####extract_data#####
setMethod("extract_data", signature(object="familiarEnsemble"),
          function(object, data, is_pre_processed=FALSE, cl=NULL,
                   time_max=waiver(),
                   aggregation_method=waiver(),
                   rank_threshold=waiver(),
                   ensemble_method=waiver(),
                   risk_ensemble_method=waiver(),
                   eval_times=waiver(),
                   metric=waiver(),
                   feature_cluster_method=waiver(),
                   feature_cluster_cut_method=waiver(),
                   feature_linkage_method=waiver(),
                   feature_similarity_metric=waiver(),
                   feature_similarity_threshold=waiver(),
                   sample_cluster_method=waiver(),
                   sample_linkage_method=waiver(),
                   sample_similarity_metric=waiver(),
                   metric_alpha=waiver(),
                   icc_type=waiver(),
                   verbose=FALSE,
                   data_element="all",
                   ...){
            # Generates a familiarData object from the ensemble.
            
            # Check whether data is a dataObject, and create one otherwise.
            if(!any(class(data) == "dataObject")){
              data <- methods::new("dataObject",
                                   data=data,
                                   is_pre_processed=is_pre_processed,
                                   outcome_type=object@outcome_type)
            }
            
            # Load models
            object <- load_models(object=object)
            
            # TEMP
            prediction_data <- extract_predictions(object=object,
                                                   data=data,
                                                   ensemble_method=ensemble_method,
                                                   time_max=time_max,
                                                   verbose=verbose)
            
            # Extract confusion matrix data.
            if(data_element %in% c("all", "confusion_matrix")){
              confusion_matrix_info <- extract_confusion_matrix(object=object,
                                                                prediction_data=prediction_data,
                                                                verbose=verbose)
            } else {
              confusion_matrix_info <- NULL
            }
            
            # Extract feature distance tables, 
            if(data_element %in% c("all", "mutual_correlation", "univariate_analysis", "feature_expressions")){
              # Not for the fs_vimp and model_vimp data elements. This is
              # because these derive cluster information from consensus
              # clustering.
              
              # Compute a table containg the pairwise distance between features.
              feature_similarity_table <- extract_feature_similarity_table(object=object,
                                                                           data=data,
                                                                           feature_similarity_metric=feature_similarity_metric,
                                                                           verbose=verbose)
            }
            
            if(data_element %in% c("all", "feature_expressions")){
              # Compute a table containing the pairwise distance between samples.
              sample_similarity_table <- extract_sample_similarity_table(object=object,
                                                                         data=data,
                                                                         sample_similarity_metric=sample_similarity_metric,
                                                                         verbose=verbose)
            }
            
            # Extract feature variable importance
            if(data_element %in% c("all", "fs_vimp")){
              fs_vimp_info <- extract_fs_vimp(object=object,
                                              aggregation_method=aggregation_method,
                                              rank_threshold=rank_threshold,
                                              verbose=verbose)
            } else {
              fs_vimp_info <- NULL
            }
            
            
            # Extract model-based variable importance
            if(data_element %in% c("all", "model_vimp")){
              model_vimp_info <- extract_model_vimp(object=object,
                                                    aggregation_method=aggregation_method,
                                                    rank_threshold=rank_threshold,
                                                    verbose=verbose)
            } else {
              model_vimp_info <- NULL
            }
            
            
            # Create mutual correlation information
            if(data_element %in% c("all", "mutual_correlation")){
              mutual_corr_info <- extract_mutual_correlation(object=object,
                                                             data=data,
                                                             feature_similarity_table=feature_similarity_table,
                                                             feature_cluster_method=feature_cluster_method,
                                                             feature_linkage_method=feature_linkage_method,
                                                             feature_similarity_metric=feature_similarity_metric,
                                                             verbose=verbose)
            } else {
              mutual_corr_info <- NULL
            }
            
            
            # Expression heatmap data
            if(data_element %in% c("all", "feature_expressions")){
              expression_info <- extract_feature_expression(object=object,
                                                            data=data,
                                                            feature_similarity_table,
                                                            sample_similarity_table,
                                                            feature_cluster_method=feature_cluster_method,
                                                            feature_linkage_method=feature_linkage_method,
                                                            feature_similarity_metric=feature_similarity_metric,
                                                            sample_cluster_method=sample_cluster_method,
                                                            sample_linkage_method=sample_linkage_method,
                                                            sample_similarity_metric=sample_similarity_metric,
                                                            eval_times=eval_times,
                                                            verbose=verbose)
            } else {
              expression_info <- NULL
            }
            
            
            # Univariate analysis
            if(data_element %in% c("all", "univariate_analysis")){
              univar_info <- extract_univariate_analysis(object=object,
                                                         data=data,
                                                         icc_type=icc_type,
                                                         feature_similarity_table=feature_similarity_table,
                                                         feature_cluster_method=feature_cluster_method,
                                                         feature_cluster_cut_method=feature_cluster_cut_method,
                                                         feature_linkage_method=feature_linkage_method,
                                                         feature_similarity_threshold=feature_similarity_threshold,
                                                         feature_similarity_metric=feature_similarity_metric,
                                                         verbose=verbose)
            } else {
              univar_info <- NULL
            }
            
            
            # Extract hyper-parameters
            if(data_element %in% c("all", "hyperparameters")){
              hyperparameter_info <- extract_hyperparameters(object=object,
                                                             verbose=verbose)
            } else {
              hyperparameter_info <- NULL
            }
            
            # Create predictions.
            if(data_element %in% c("all", "prediction_data", "model_performance",
                                   "stratification_data", "auc_data", "confusion_matrix")){
              prediction_data <- extract_predictions(object=object,
                                                     data=data,
                                                     ensemble_method=ensemble_method,
                                                     time_max=time_max,
                                                     verbose=verbose)
            } else {
              prediction_data <- NULL
            }
            
            # Compute model performance based on the prediction_data
            if(data_element %in% c("all", "model_performance")){
              model_performance_data <- extract_performance(object=object,
                                                            prediction_data=prediction_data,
                                                            metric=metric,
                                                            metric_alpha=metric_alpha,
                                                            verbose=verbose)
            } else {
              model_performance_data <- NULL
            }
            
            # Extract information regarding stratification
            if(data_element %in% c("all", "kaplan_meier_info")){
              km_info <- extract_km_cutoffs(object=object,
                                            verbose=verbose)
            } else {
              km_info <- NULL
            }
            
            # Compute log-rank tests
            if(data_element %in% c("all", "kaplan_meier_data")){
              stratification_data <- extract_stratification_data(object=object,
                                                                 prediction_data=prediction_data,
                                                                 risk_ensemble_method=risk_ensemble_method,
                                                                 time_max=time_max,
                                                                 verbose=verbose)
            } else {
              stratification_data <- NULL
            }
            
            # Extract calibration data
            if(data_element %in% c("all", "calibration_data")){
              calibration_data <- extract_calibration_data(object=object,
                                                           data=data,
                                                           eval_times=eval_times,
                                                           verbose=verbose)
            } else {
              calibration_data <- NULL
            }
            
            # Extract AUC data
            if(data_element %in% c("all", "auc_data")){
              auc_data <- extract_auc_data(object=object,
                                           prediction_data=prediction_data,
                                           metric_alpha=metric_alpha,
                                           verbose=verbose)
            } else {
              auc_data <- NULL
            }
            
            # Set up a placehold pooling table. This may need to be adepted.
            pooling_table <- data.table::data.table("ensemble_data_id"=object@run_table$ensemble_data_id,
                                                    "ensemble_run_id"=object@run_table$ensemble_run_id,
                                                    "data_perturb_level"=ifelse(is.na(data@perturb_level), 0, data@perturb_level),
                                                    "pool_data_id"=0L,
                                                    "pool_run_id"=0,
                                                    "pool_perturb_level"=0)
            
            # Create a familiarData object
            fam_data <- methods::new("familiarData",
                                     outcome_type = object@outcome_type,
                                     class_levels = object@class_levels,
                                     fs_vimp = fs_vimp_info,
                                     model_vimp = model_vimp_info,
                                     hyperparameters = hyperparameter_info,
                                     req_feature_cols = object@req_feature_cols,
                                     important_features = object@important_features,
                                     learner = object@learner,
                                     fs_method = object@fs_method,
                                     pooling_table = pooling_table,
                                     prediction_data = prediction_data,
                                     calibration_info = add_model_name(object@calibration_info, object=object),
                                     calibration_data = calibration_data,
                                     model_performance = model_performance_data,
                                     km_info = km_info,
                                     km_data = stratification_data,
                                     auc_data = auc_data,
                                     univariate_analysis = univar_info,
                                     feature_expressions = expression_info,
                                     mutual_correlation = mutual_corr_info,
                                     is_anonymised = FALSE,
                                     is_validation = data@load_validation,
                                     generating_ensemble = get_object_name(object=object, abbreviated=FALSE),
                                     project_id = object@project_id)
            
            # Add package version to the data set 
            fam_data <- add_package_version(object=fam_data)
            
            # Return data
            return(fam_data)
          })

#'@title Internal function to extract feature selection variable importance.
#'
#'@description Aggregate variable importance obtained during feature selection.
#'  This information can only be obtained as part of the main `summon_familiar` process.
#'
#'@inheritParams extract_data
#'
#'@return A list containing feature selection variable importance information.
#'@md
#'@keywords internal
setGeneric("extract_fs_vimp", function(object,
                                       aggregation_method=waiver(),
                                       rank_threshold=waiver(),
                                       verbose=FALSE, ...) standardGeneric("extract_fs_vimp"))

#####extract_fs_vimp#####
setMethod("extract_fs_vimp", signature(object="familiarEnsemble"),
          function(object,
                   aggregation_method=waiver(),
                   rank_threshold=waiver(),
                   verbose=FALSE){
            
            # Obtain aggregation method from stored settings, if required.
            if(is.waive(aggregation_method)){
              aggregation_method <- object@settings$aggregation
            }
            
            # Check aggregation method
            rank.check_aggregation_method(method=aggregation_method)
            
            # Obtain rank thresholds from stored settings, if required
            if(is.waive(rank_threshold)){
              rank_threshold <- object@settings$aggr_rank_threshold
            }
            
            # Check rank threshold. NULL is an allowed value.
            if(!is.null(rank_threshold)){
              .check_number_in_valid_range(x=rank_threshold, var_name="rank_threshold", range=c(1, Inf))
            }
            
            # Load project list and file_paths
            file_paths <- tryCatch({get_file_paths()}, error=function(err)(return(NULL)))
            project_list <- tryCatch({get_project_list()}, error=function(err)(return(NULL)))
            
            if(is.null(file_paths) | is.null(project_list)){
              return(list("vimp_table" = NULL,
                          "aggregation_method" = aggregation_method,
                          "rank_threshold" = rank_threshold))
            }
            
            # Message extraction start
            if(verbose){
              logger.message(paste0("\tExtracting variable importance obtained during feature selection."))
            }
            
            # Define the run table -> at the pooling level
            run <- getRunList(iter_list=project_list$iter_list,
                              data_id=object@run_table$ensemble_data_id,
                              run_id=object@run_table$ensemble_run_id)
            
            # Obtain variable importance table
            vimp_table <- rank.get_vimp_table(run=run,
                                              fs_method=object@fs_method,
                                              proj_list=project_list,
                                              file_paths=file_paths,
                                              decluster=TRUE)
            
            # Find the translation_table by performing consensus clustering.
            translation_table <- rank.consensus_clustering(vimp_table=vimp_table)
            
            # Extract variable importance information
            fs_vimp <- rank.get_feature_ranks(vimp_table=vimp_table,
                                              translation_table=translation_table,
                                              aggregation_method=aggregation_method,
                                              rank_threshold=rank_threshold,
                                              decluster=TRUE)
            
            # Extract feature occurrence within top 5.
            occurrence_vimp <- rank.get_feature_ranks(vimp_table=vimp_table,
                                                      translation_table=translation_table,
                                                      aggregation_method="stability",
                                                      rank_threshold=5,
                                                      decluster=TRUE)
            
            # Remove the aggr_rank and cluster_id column and rename the aggr_score column.
            occurrence_vimp[, ":="("aggr_rank"=NULL, "cluster_id"=NULL)]
            data.table::setnames(occurrence_vimp, "aggr_score", "occurrence")
            
            # Merge occurrence into the main variable importance table based on feature name.
            if(is_empty(occurrence_vimp)){
              fs_vimp[, "occurrence":=numeric(0)]
            } else {
              fs_vimp <- merge(x=fs_vimp, y=occurrence_vimp, on="name", all=TRUE)
            }
            
            # Update names
            data.table::setnames(fs_vimp, c("aggr_rank", "aggr_score"), c("rank", "score"))
            fs_vimp <- fs_vimp[order(rank)]
            
            # Add cluster size.
            fs_vimp[, "cluster_size":=.N, by="cluster_id"]
            
            # Set column order
            data.table::setcolorder(fs_vimp, neworder=c("name", "score", "rank", "occurrence", "cluster_id", "cluster_size"))
            
            # Store vimp information with settings used to determine the ranks
            vimp_info <- list("vimp_table" = fs_vimp,
                              "aggregation_method" = aggregation_method,
                              "rank_threshold" = rank_threshold)
            
            return(vimp_info)
          })



#'@title Internal function to extract variable importance from models.
#'
#'@description Aggregate variable importance from models in a
#'  `familiarEnsemble`.
#'
#'@param feature_similarity_table Table containing pairwise distance between
#'  features. This is used to determine cluster information, and indicate
#'  co-clustered important features. The table is created by the
#'  `extract_feature_similarity_table` method.
#'@inheritParams extract_data
#'
#'@return A list containing variable importance information.
#'@md
#'@keywords internal
setGeneric("extract_model_vimp", function(object,
                                          aggregation_method=waiver(),
                                          rank_threshold=waiver(),
                                          verbose=FALSE,
                                          ...) standardGeneric("extract_model_vimp"))

#####extract_model_vimp (ensemble)#####
setMethod("extract_model_vimp", signature(object="familiarEnsemble"),
          function(object,
                   aggregation_method=waiver(),
                   rank_threshold=waiver(),
                   verbose=FALSE,
                   ...){
            
            # Message extraction start
            if(verbose){
              logger.message(paste0("\tExtracting variable importance obtained from the models."))
            }
            
            # Test if models are properly loaded
            if(!is_model_loaded(object=object)){
              ..error_ensemble_models_not_loaded()
            }
            
            # Obtain aggregation method from stored settings, if required.
            if(is.waive(aggregation_method)){
              aggregation_method <- object@settings$aggregation
            }
            
            # Check aggregation method
            rank.check_aggregation_method(method=aggregation_method)
            
            # Obtain rank thresholds from stored settings, if required
            if(is.waive(rank_threshold)){
              rank_threshold <- object@settings$aggr_rank_threshold
            }
            
            # Check rank threshold. NULL is an allowed value.
            if(!is.null(rank_threshold)){
              .check_number_in_valid_range(x=rank_threshold, var_name="rank_threshold", range=c(1, Inf))
            }
            
            # Suppress NOTES due to non-standard evaluation in data.table
            rank <- NULL
            
            # Get variable importance from individual models
            raw_model_vimp <- data.table::rbindlist(lapply(object@model_list, extract_model_vimp))
            
            # Check for empty model_vimp
            if(nrow(raw_model_vimp) == 0){
              return(list("vimp_table" = NULL,
                          "aggregation_method" = aggregation_method,
                          "rank_threshold" = rank_threshold))
            }
            
            
            # Find translation table.
            translation_table <- rank.consensus_clustering(vimp_table=raw_model_vimp)
            
            # Re-cluster variables so that each cluster of features will only be counted
            # once. This is particularly helpful for features that incorporate
            # occurrence or use a threshold for the important set of features.
            raw_model_vimp <- rank.recluster_vimp_table(vimp_table=raw_model_vimp,
                                                        translation_table=translation_table)
            
            # Determine aggregate ranks
            model_vimp <- rank.aggregate_feature_ranks(dt_vimp=raw_model_vimp,
                                                       aggregation_method=aggregation_method,
                                                       rank_threshold=rank_threshold)
            
            # Extract feature occurrence within top 5.
            occurrence_vimp <- rank.aggregate_feature_ranks(dt_vimp=raw_model_vimp,
                                                            aggregation_method="stability",
                                                            rank_threshold=5)
            
            # Remove the aggr_rank column and rename the aggr_score column.
            occurrence_vimp[, ":="("aggr_rank"=NULL)]
            data.table::setnames(occurrence_vimp, "aggr_score", "occurrence")
            
            # Merge occurrence into the main variable importance table based on feature name.
            if(is_empty(occurrence_vimp)){
              model_vimp[, "occurrence":=numeric(0)]
            } else {
              model_vimp <- merge(x=model_vimp, y=occurrence_vimp, on="name", all=TRUE)
            }
            
            # Replace clusters by individual features
            model_vimp <- rank.decluster_vimp_table(vimp_table=model_vimp,
                                                    translation_table=translation_table)
            
            # Update column names
            data.table::setnames(model_vimp, c("aggr_rank", "aggr_score"), c("rank", "score"))
            model_vimp <- model_vimp[order(rank)]
            
            # Add cluster size.
            model_vimp[, "cluster_size":=.N, by="cluster_id"]

            # Store vimp information with settings used to determine the ranks
            vimp_info <- list("vimp_table" = model_vimp,
                              "aggregation_method" = aggregation_method,
                              "rank_threshold" = rank_threshold)
            
            # Return variable importance information
            return(vimp_info)
          })

#####extract_model_vimp (model)#####
setMethod("extract_model_vimp", signature(object="familiarModel"),
          function(object, ...){
            
            # Extract variable importance from the models
            model_vimp <- learner.main(object=object, purpose="vimp")
            
            # Add the data_id and run_id column
            model_vimp[, ":="("data_id"=tail(object@run_table, n=1)$data_id,
                              "run_id"=tail(object@run_table, n=1)$run_id)]
            
            # Decluster data
            model_vimp <- rank.decluster_vimp_table(vimp_table=model_vimp, feature_info_list=object@feature_info)
            
            return(model_vimp)
          })


#'@title Internal function to extract hyperparameters from models.
#'
#'@description Collects hyperparameters from models in a `familiarEnsemble`.
#'
#'@inheritParams extract_data
#'
#'@return A `data.table` with hyperparameters.
#'@md
#'@keywords internal
setGeneric("extract_hyperparameters", function(object,
                                               verbose=FALSE,
                                               ...) standardGeneric("extract_hyperparameters"))

#####extract_hyperparameters#####
setMethod("extract_hyperparameters", signature(object="familiarEnsemble"),
          function(object,
                   verbose=FALSE){
            # Extracts hyper-parameters from each model and collects them.
            
            # Message extraction start
            if(verbose){
              logger.message(paste0("\tExtracting hyperparameters from the models in the ensemble."))
            }
            
            # Test if models are properly loaded
            if(!is_model_loaded(object=object)){
              ..error_ensemble_models_not_loaded()
            }
            
            # Collect hyperparameters
            hyperparameter_info <- data.table::rbindlist(lapply(object@model_list, function(fam_model){
              # Parse as data.table
              data <- data.table::as.data.table(fam_model@hyperparameters)
              
              # Add model name column
              data <- add_model_name(data=data, object=fam_model)
              
              return(data)
            } ))
            
            return(hyperparameter_info)
          })



#'@title Internal function to extract predicted values from models.
#'
#'@description Collects predicted values from models in a `familiarEnsemble`.
#'
#'@inheritParams extract_data
#'
#'@return A list with single-model and ensemble predictions.
#'@md
#'@keywords internal
setGeneric("extract_predictions",
           function(object,
                    data,
                    ensemble_method=waiver(),
                    time_max=waiver(),
                    verbose=FALSE,
                    ...) standardGeneric("extract_predictions"))

#####extract_predictions#####
setMethod("extract_predictions", signature(object="familiarEnsemble"),
          function(object,
                   data,
                   ensemble_method=waiver(),
                   time_max=waiver(),
                   verbose=FALSE,
                   ...){
            # Extract predictions from the data using the models in the
            # ensemble. Note: we do not call the predict function on the
            # familiarEnsemble directly as this would cause predict to become
            # highly convoluted, in particular with generating both single and
            # ensemble predictions.
            
            # Message extraction start
            if(verbose){
              logger.message(paste0("\tComputing ensemble predictions for the dataset."))
            }
            
            # Load time_max from the object settings attribute, if it is not provided.
            if(is.waive(time_max) & object@outcome_type %in% c("survival")){
              time_max <- object@settings$time_max
            }
            
            # Check time_max argument
            if(object@outcome_type %in% c("survival")){
              .check_number_in_valid_range(time_max, var_name="time_max", range=c(0.0, Inf), closed=c(FALSE, TRUE))
            }
            
            # Obtain ensemble method from stored settings, if required.
            if(is.waive(ensemble_method)){
              ensemble_method <- object@settings$ensemble_method
            }
            
            # Check ensemble_method argument
            .check_parameter_value_is_valid(x=ensemble_method, var_name="ensemble_method",
                                            values=.get_available_ensemble_prediction_methods())
            
            # Test if models are properly loaded
            if(!is_model_loaded(object=object)){
              ..error_ensemble_models_not_loaded()
            }
            
            # Aggregate data. It does not make sense to keep duplicate rows here.
            data <- aggregate_data(data=data)
            
            # Predict for all separate models
            prediction_data <- list("single"=lapply(object@model_list, function(object, newdata){
              
              # Make predictions
              predict_table <- predict(object=object, newdata=newdata, allow_recalibration=TRUE, extra_output=FALSE, time_max=time_max)
              
              # Add the model name to the table
              predict_table <- add_model_name(data=predict_table, object=object)
              
              return(predict_table)
            }, newdata=data))
            
            # Create the ensemble predictions
            prediction_data$ensemble <- ensemble_prediction(object=object, prediction_data=data.table::rbindlist(prediction_data$single), ensemble_method=ensemble_method)
            
            # Add the ensemble name to the table
            prediction_data$ensemble <- add_model_name(data=prediction_data$ensemble, object=object)
            
            return(prediction_data)
          })


#'@title Internal function to extract Kaplan-Meier risk group stratification cutoffs from models.
#'
#'@description Collects Kaplan-Meier risk group stratification cutoffs from
#'  models in a `familiarEnsemble`.
#'
#'@inheritParams extract_data
#'
#'@return A data.table with risk group cutoffs.
#'@md
#'@keywords internal
setGeneric("extract_km_cutoffs", function(object,
                                          verbose=FALSE,
                                          ...) standardGeneric("extract_km_cutoffs"))

#####extract_km_cutoffs#####
setMethod("extract_km_cutoffs", signature(object="familiarEnsemble"),
          function(object,
                   verbose=FALSE){
            
            # Test if the outcome type is survival
            if(!object@outcome_type %in% c("survival")){
              return(NULL)
            }
            
            # Message extraction start
            if(verbose){
              logger.message(paste0("\tExtracting Kaplan-Meier cutoffs from the models."))
            }
            
            # Test if models are properly loaded
            if(!is_model_loaded(object=object)){
              ..error_ensemble_models_not_loaded()
            }
            
            # Collect Kaplan-Meier stratification parameters
            km_cut_off_info <- data.table::rbindlist(lapply(object@model_list, function(fam_model){
              
              if(is_empty(fam_model@km_info$parameters)){
                return(NULL)
              }
              
              # Iterate over stratification parameters
              data <- data.table::rbindlist(lapply(fam_model@km_info$parameters, function(method_list){
                
                # Extract information
                data <- data.table::data.table("stratification_method"=method_list$method, "cutoff"=method_list$cutoff,
                                               "index"=seq_len(length(method_list$cutoff)))
                
                return(data)
              }))
              
              # Add model name column
              data <- add_model_name(data=data, object=fam_model)
              
              return(data)
            } ))
            
            return(km_cut_off_info)
          })


#'@title Internal function to extract performance metrics.
#'
#'@description Computes and collects discriminative performance metrics from a
#'  `familiarEnsemble`.
#'
#'@param prediction_data A table with pre-computed predicted values per sample.
#'  This table is generated by the `extract_predictions` method. If
#'  `prediction_data` is not provided it will be computed internally.
#'
#'@inheritParams extract_data
#'@inheritDotParams extract_predictions
#'
#'@details This method differs from the `assess_performance` method that
#'  performance predictions on the fly and enables computation of objective
#'  scores.
#'
#'  This function also computes credibility intervals for the ensemble model, at
#'  the level of `metric_alpha`. This is a general method. Metrics with known,
#'  theoretically derived confidence intervals, nevertheless have a credibility
#'  interval computed.
#'
#'@return A list with data.tables for single and ensemble model assessments.
#'@md
#'@keywords internal
setGeneric("extract_performance",
           function(object,
                    prediction_data=NULL, 
                    metric=waiver(),
                    metric_alpha=waiver(),
                    verbose=FALSE,
                    ...) standardGeneric("extract_performance"))

#####extract_performance#####
setMethod("extract_performance", signature(object="familiarEnsemble", prediction_data="list"),
          function(object,
                   prediction_data=NULL,
                   metric=waiver(),
                   metric_alpha=waiver(),
                   verbose=FALSE,
                   ...){
            # Compute metrics based on a pre-calculated set of prediction_data.
            # Note that this is different from the "assess_performance" method,
            # which performs the prediction on the fly, and enables the
            # extraction of objective scores.
            
            # Check if predictions were generated, and generate them on the fly
            # if necessary.
            if(is.null(prediction_data)){
              prediction_data <- do.call(extract_predictions, args=append(list("object"=object),
                                                                          list(...)))
            }
            
            # Load metric(s) from the object settings attribute if not provided
            # externally.
            if(is.waive(metric)){
              metric <- object@settings$metric
            }
            
            # Load confidence alpha from object settings attribute if not
            # provided externally.
            if(is.waive(metric_alpha)){
              metric_alpha <- object@settings$metric_alpha
            }
            
            # Check metric input argument
            sapply(metric, metric.check_outcome_type, outcome_type=object@outcome_type)
            
            # Check metric_alpha input argument
            .check_number_in_valid_range(x=metric_alpha, var_name="metric_alpha",
                                         range=c(0.0, 1.0), closed=c(FALSE, FALSE))
            
            
            ##### Compute the performance metrics for single models #####
            performance_list <- list("single"=data.table::rbindlist(lapply(seq_len(length(prediction_data$single)), function(ii, object, prediction_data, metric) {
              
              # Calculate the model performance
              model_performance <- .process_single_iter_performance(prediction_table=prediction_data$single[[ii]],
                                                                    metric=metric,
                                                                    learner=object@learner,
                                                                    outcome_type=object@outcome_type,
                                                                    samples=NULL)
              
              # Add the model name
              model_performance <- add_model_name(data=model_performance, object=object@model_list[[ii]])
              
              return(model_performance)
            }, object=object, prediction_data=prediction_data, metric=metric)))
            
            ##### Compute the performance metrics for ensemble models #####
            
            # Determine the desired number of iterations for obtaining a stable
            # confidence interval.
            n_iter <- ceiling(20 / metric_alpha)
            
            # Get ensemble performance.
            performance_list$ensemble <- data.table::rbindlist(lapply(seq_len(n_iter), function(ii, prediction_table, metric, learner, outcome_type){
              
              # Generate a bootstrap sample.
              samples <- sample(x=prediction_table$subject_id, size=nrow(prediction_table), replace=TRUE)
              
              # Calculate the model performance
              model_performance <- .process_single_iter_performance(prediction_table=prediction_table,
                                                                    metric=metric,
                                                                    learner=learner,
                                                                    outcome_type=outcome_type,
                                                                    samples=samples)
              
              return(model_performance)
            }, prediction_table=prediction_data$ensemble, metric=metric, learner=object@learner, outcome_type=object@outcome_type))
            
            # Add model name to ensemble performance
            performance_list$ensemble <- add_model_name(data=performance_list$ensemble, object=object)
            
            # Add the metrics and the alpha-level used
            performance_list$metric <- metric
            performance_list$conf_alpha <- metric_alpha
            
            return(performance_list)
          })

#'@title Internal function to extract area under the ROC curve information.
#'
#'@description Computes the ROC curve from a `familiarEnsemble`.
#'
#'@param prediction_data A table with pre-computed predicted values per sample.
#'  This table is generated by the `extract_predictions` method. If
#'  `prediction_data` is not provided it will be computed internally.
#'
#'@inheritParams extract_data
#'@inheritDotParams extract_predictions
#'
#'@details This function also computes credibility intervals for the ROC curve
#'  for the ensemble model, at the level of `metric_alpha`. In the case of
#'  multinomial outcomes, an AUC curve is computed per class in a
#'  one-against-all fashion.
#'
#'  To allow plotting of multiple AUC curves in the same plot and the use of
#'  ensemble models, the AUC curve is evaluated at 0.01 (1-specificity) intervals.
#'
#'@return A list with data.tables for single and ensemble model ROC curve data.
#'@md
#'@keywords internal
setGeneric("extract_auc_data", function(object,
                                        prediction_data=NULL,
                                        metric_alpha=waiver(),
                                        verbose=FALSE,
                                        ...) standardGeneric("extract_auc_data"))

#####extract_auc_data#####
setMethod("extract_auc_data", signature(object="familiarEnsemble"),
          function(object,
                   prediction_data=NULL,
                   metric_alpha=waiver(),
                   verbose=FALSE,
                   ...) {
            # Extract data for plotting AUC curves.
            
            outcome_type <- object@outcome_type
            
            # AUC data can only be prepared for binomial and multinomial outcomes
            if(!outcome_type %in% c("binomial", "multinomial")){
              return(NULL)
            }
            
            # Message start of auc computations
            if(verbose){
              logger.message(paste0("\tComputing receiver-operating characteristic curves."))
            }
            
            # Obtain alpha from the 
            if(is.waive(metric_alpha)){
              metric_alpha <- object@settings$metric_alpha
            }
            
            # Check alpha
            .check_number_in_valid_range(metric_alpha, var_name="metric_alpha", range=c(0.0, 1.0), closed=c(FALSE, FALSE))
            
            # Check if predictions were generated, and generate them on the fly
            # if necessary.
            if(is.null(prediction_data)){
              prediction_data <- do.call(extract_predictions, args=append(list("object"=object),
                                                                          list(...)))
            }
            
            ##### Compute AUC curves for single models #####
            auc_table_list <- list("single"=data.table::rbindlist(lapply(seq_len(length(prediction_data$single)), function(ii, object, prediction_data) {
              outcome_type <- object@outcome_type
              
              if(outcome_type == "binomial"){
                # Use the second class as positive class, as per standard.
                class_levels <- object@class_levels[2]
              } else {
                # Use all classes as positive class in case of multinomial data
                class_levels <- object@class_levels
              }
              
              # Extract basic information concerning matching entries
              basic_auc_table_list <- lapply(class_levels, .prepare_basic_auc_table , prediction_table=prediction_data$single[[ii]], outcome_type=outcome_type)
              
              # Create a complete AUC table from the data. No confidence intervals are calculated here.
              single_auc_table <- data.table::rbindlist(lapply(basic_auc_table_list, .complete_auc_table, compute_confidence_intervals=FALSE))
              
              # Convert the pos_class into a factor for multinomial outcomes.
              if(outcome_type == "multinomial"){
                single_auc_table$pos_class <- factor(single_auc_table$pos_class, levels=object@class_levels)
              }
              
              # Add the model name
              single_auc_table <- add_model_name(data=single_auc_table, object=object@model_list[[ii]])
              
              return(single_auc_table)
            }, object=object, prediction_data=prediction_data)))
            
            
            ##### Compute AUC curves for the ensemble model #####
            if(outcome_type == "binomial"){
              # Use the second class as positive class, as per standard.
              class_levels <- object@class_levels[2]
            } else {
              # Use all classes as positive class in case of multinomial data
              class_levels <- object@class_levels
            }
            
            # Extract basic information concerning matching entries
            basic_auc_table_list <- lapply(class_levels, .prepare_basic_auc_table , prediction_table=prediction_data$ensemble, outcome_type=outcome_type)
            
            # Create a complete AUC table from the data. No confidence intervals are calculated here.
            ensemble_auc_table <- data.table::rbindlist(lapply(basic_auc_table_list, .complete_auc_table, compute_confidence_intervals=TRUE))
            
            # Convert the pos_class into a factor for multinomial outcomes.
            if(outcome_type == "multinomial"){
              ensemble_auc_table$pos_class <- factor(ensemble_auc_table$pos_class, levels=object@class_levels)
            }
            
            # Add the model name
            auc_table_list$ensemble <- add_model_name(data=ensemble_auc_table, object=object)
            
            # Add the confidence alpha level
            auc_table_list$conf_alpha <- metric_alpha
            
            return(auc_table_list)
          })



#'@title Internal function to extract calibration data.
#'
#'@description Computes calibration data from a `familiarEnsemble` object.
#'  Calibration tests are performed based on expected (predicted) and observed
#'  outcomes. For all outcomes, calibration-at-the-large and calibration slopes
#'  are determined. Furthermore, for all but survival outcomes, a repeated,
#'  randomised grouping Hosmer-Lemeshow test is performed. For survival
#'  outcomes, the Nam-D'Agostino and Greenwood-Nam-D'Agostino tests are
#'  performed.
#'
#'@inheritParams extract_data
#'
#'@return A list with data.tables containing calibration test information for
#'  the ensemble model.
#'@md
#'@keywords internal
setGeneric("extract_calibration_data",
           function(object,
                    data,
                    eval_times=waiver(),
                    is_pre_processed=FALSE,
                    verbose=FALSE,
                    ...) standardGeneric("extract_calibration_data"))

#####extract_calibration_data#####
setMethod("extract_calibration_data", signature(object="familiarEnsemble"),
          function(object,
                   data,
                   eval_times=waiver(),
                   is_pre_processed=FALSE,
                   verbose=FALSE,
                   ...){
            
            # Message extraction start
            if(verbose){
              logger.message(paste0("\tAssessing model calibration."))
            }
            
            # Load eval_times from the object settings attribute, if it is not provided.
            if(is.waive(eval_times)){
              eval_times <- object@settings$eval_times
            }
            
            # Check eval_times argument
            if(object@outcome_type %in% c("survival")){
              sapply(eval_times, .check_number_in_valid_range, var_name="eval_times", range=c(0.0, Inf), closed=c(FALSE, TRUE))
            }
            
            # Test if models are properly loaded
            if(!is_model_loaded(object=object)){
              ..error_ensemble_models_not_loaded()
            }
            
            # This function is the same for familiarModel and familiarEnsemble objects
            return(assess_calibration(object=object, data=data, eval_times=eval_times, is_pre_processed=is_pre_processed))
          })


#'@title Internal function to extract stratification data.
#'
#'@description Computes and extracts stratification data from a
#'  `familiarEnsemble` object. This includes the data required to draw
#'  Kaplan-Meier plots, as well as logrank and hazard-ratio tests between the
#'  respective risk groups.
#'
#'@inheritParams extract_data
#'
#'@return A list with data.tables containing information concerning risk group
#'  stratification.
#'@md
#'@keywords internal
setGeneric("extract_stratification_data", function(object,
                                                   data=NULL,
                                                   prediction_data=NULL,
                                                   ensemble_method=waiver(),
                                                   time_max=waiver(),
                                                   risk_group_list=NULL,
                                                   risk_ensemble_method=waiver(),
                                                   verbose=FALSE,
                                                   ...) standardGeneric("extract_stratification_data"))
#####extract_stratification_data#####
setMethod("extract_stratification_data", signature(object="familiarEnsemble"),
          function(object,
                   data=NULL,
                   prediction_data=NULL,
                   ensemble_method=waiver(),
                   time_max=waiver(),
                   risk_group_list=NULL,
                   risk_ensemble_method=waiver(),
                   verbose=FALSE,
                   ...){
            
            # Only assess stratification for survival outcomes.
            if(!object@outcome_type %in% c("survival")){
              return(NULL)
            }
            
            # Message extraction start
            if(verbose){
              logger.message(paste0("\tAssessing stratification into risk groups."))
            }
            
            # Assess stratification
            return(assess_stratification(object=object, data=data, prediction_data=prediction_data,
                                         ensemble_method=ensemble_method, time_max=time_max,
                                         risk_group_list=risk_group_list,
                                         risk_ensemble_method=risk_ensemble_method))
          })


#'@title Internal function to extract data from a univariate analysis.
#'
#'@description Computes and extracts univariate analysis for the features used
#'  in a `familiarEnsemble` object. This assessment includes the computation of
#'  p and q-values, as well as robustness (in case of repeated measurements).
#'
#'@inheritParams extract_data
#'
#'@return A list with a data.table containing information concerning the
#'  univariate analysis of important features.
#'@md
#'@keywords internal
setGeneric("extract_univariate_analysis", function(object,
                                                   data,
                                                   icc_type=waiver(),
                                                   feature_similarity_table=NULL,
                                                   feature_cluster_method=waiver(),
                                                   feature_cluster_cut_method=waiver(),
                                                   feature_linkage_method=waiver(),
                                                   feature_similarity_threshold=waiver(),
                                                   feature_similarity_metric=waiver(),
                                                   verbose=FALSE,
                                                   ...) standardGeneric("extract_univariate_analysis"))

#####extract_univariate_analysis#####
setMethod("extract_univariate_analysis", signature(object="familiarEnsemble", data="ANY"),
          function(object,
                   data,
                   icc_type=waiver(),
                   feature_similarity_table=NULL,
                   feature_cluster_method=waiver(),
                   feature_cluster_cut_method=waiver(),
                   feature_linkage_method=waiver(),
                   feature_similarity_threshold=waiver(),
                   feature_similarity_metric=waiver(),
                   verbose=FALSE){
            
            # Message extraction start
            if(verbose){
              logger.message(paste0("\tExtracting univariate analysis information."))
            }
            
            # Obtain cluster method from stored settings, if required.
            if(is.waive(feature_cluster_method)){
              feature_cluster_method <- object@settings$feature_cluster_method
            }
            
            # Obtain linkage function from stored settings, if required.
            if(is.waive(feature_linkage_method)){
              feature_linkage_method <- object@settings$feature_linkage_method
            }
            
            # Obtain feature cluster cut method from stored settings, if required.
            if(is.waive(feature_cluster_cut_method)){
              feature_cluster_cut_method <- object@settings$feature_cluster_cut_method
            }
            
            # Obtain cluster similarity threshold from stored settings, if required.
            if(is.waive(feature_similarity_threshold)){
              feature_similarity_threshold <- object@settings$feature_similarity_threshold
            }
            
            # Obtain similarity metric from stored settings, if required.
            if(is.waive(feature_similarity_metric)){
              feature_similarity_metric <- object@settings$feature_similarity_metric
            }
            
            # Replace feature cluster method == "none" with "hclust"
            if(feature_cluster_method == "none"){
              feature_cluster_method <- "hclust"
            }
            
            .check_cluster_parameters(cluster_method=feature_cluster_method,
                                      cluster_linkage=feature_linkage_method,
                                      cluster_cut_method=feature_cluster_cut_method,
                                      cluster_similarity_threshold=feature_similarity_threshold,
                                      cluster_similarity_metric=feature_similarity_metric,
                                      var_type="feature")
            
            # Suppress NOTES due to non-standard evaluation in data.table
            p_value <- NULL
            
            # Get and process the input data
            data <- process_input_data(object=object, data=data, stop_at="normalisation")
            
            # Determine some of the processing settings
            if(!all(data@data$repetition_id==1)){
              is_repeated_measurement <- TRUE
              
              # Obtain from settings, if unset.
              if(is.waive(icc_type)){
                icc_type <- object@settings$icc_type
              }
              
              # Check icc_type
              .check_parameter_value_is_valid(x=icc_type, var_name="icc_type",
                                              values=.get_available_icc_types())
            } else {
              is_repeated_measurement <- FALSE
            }
            
            has_qvalue_package <- is_package_installed(name="qvalue", verbose=FALSE)

            # Define an empty prototype table
            empty_table <- data.table::data.table("model_name"=character(0),
                                                  "name"=character(0),
                                                  "cluster_id"=integer(0),
                                                  "cluster_size"=integer(0),
                                                  "p_value"=numeric(0),
                                                  "p_value_corrected"=numeric(0))
            
            if(has_qvalue_package){
              empty_table <- cbind(empty_table, data.table::data.table("q_value"=numeric(0)))
            }
            
            if(is_repeated_measurement) {
              empty_table <- cbind(empty_table, data.table::data.table("icc"=numeric(0),
                                                                       "icc_low"=numeric(0),
                                                                       "icc_up"=numeric(0),
                                                                       "icc_panel"=numeric(0),
                                                                       "icc_panel_low"=numeric(0),
                                                                       "icc_panel_up"=numeric(0)))
            }
            
            # Check if the data object is empty -- return an empty table if this is the case
            if(is_empty(data)){
              return(list("data"=empty_table))
            }
            
            # Check if the number of samples is sufficient (>5), and return an empty table if not.
            if(data.table::uniqueN(data@data, by="subject_id") <= 5){
              return(list("data"=empty_table))
            }
            
            # Maintain only important features. The current set is based on the required features.
            data <- filter_features(data=data, available_features=object@important_features)
            
            # Determine feature columns
            feature_columns <- get_feature_columns(x=data)
            
            # Check if there are any features in the model.
            if(length(feature_columns) == 0) return(list("data"=empty_table))
            
            # Calculate univariate P values, based on aggregated data
            regr_p_values <- compute_univariable_p_values(cl=NULL,
                                                          data_obj=aggregate_data(data=data),
                                                          feature_columns=feature_columns)
            
            # Find and replace non-finite values
            regr_p_values[!is.finite(regr_p_values)] <- 1.0
            
            # Collect to table
            univariate_data <- data.table::data.table("name"=names(regr_p_values),
                                                      "p_value"=regr_p_values)[order(p_value)]
            univariate_data[, "p_value_corrected":=stats::p.adjust(p_value, method="fdr")]
            
            # Only introduce q-values if the qvalue package is installed
            if(has_qvalue_package){
              
              # q-values can only be computed for larger numbers of features
              computed_q_value <- tryCatch({
                qvalue::qvalue(p=univariate_data$p_value)$qvalues
              }, error=function(err){
                return(NA_real_)
              })
              
              # Set q-value
              univariate_data[, "q_value":=computed_q_value]
            }
            
            # Determine whether robustness data can be added
            # Check if repeated measurements are present, otherwise return no feature names.
            if(is_repeated_measurement){
              
              # Calculate intraclass correlation coefficient (1,1)
              icc_list <- lapply(feature_columns, function(curr_feat, data, icc_type){
                compute_icc(dt=data[, c(curr_feat, "subject_id", "cohort_id", "repetition_id"), with=FALSE],
                            type=icc_type,
                            feat_name=curr_feat)
              }, data=data@data, icc_type=icc_type)
              
              # Merge with univariate_data
              univariate_data <- merge(x=univariate_data, y=data.table::rbindlist(icc_list), by="name", all.x=TRUE)
            }
            
            # Find cluster info
            if(!is_empty(feature_similarity_table)){
              
              # Compute the distance matrix
              distance_matrix <- cluster.get_distance_matrix(similarity_table=feature_similarity_table,
                                                             similarity_metric=feature_similarity_metric)
              
              # Find cluster information
              cluster_info <- cluster.get_cluster_table(distance_matrix=distance_matrix,
                                                        require_representation=FALSE,
                                                        cluster_method=feature_cluster_method,
                                                        cluster_linkage=feature_linkage_method,
                                                        cluster_cut_method=feature_cluster_cut_method,
                                                        cluster_similarity_threshold=feature_similarity_threshold,
                                                        cluster_similarity_metric=feature_similarity_metric)
              
              # Keep only name and cluster_id columns
              cluster_info <- cluster_info[, c("name", "cluster_id"), with=FALSE]
              
              # Compute cluster size
              cluster_info[, "cluster_size":=.N, by="cluster_id"]
              
              # Merge into model_vimp
              univariate_data <- merge(univariate_data, cluster_info, by="name")
              
            } else {
              # Add singular clusters
              univariate_data[, ":="("cluster_id"=.I, "cluster_size"=1L)]
            }
            
            # Add model name
            univariate_data <- add_model_name(data=univariate_data, object=object)
            
            # Package to list
            univariate_info <- list("data"=univariate_data)
            
            # Add ICC type if it is used.
            if(is_repeated_measurement){
              univariate_info$icc_type <- icc_type
            }

            return(univariate_info)
          })


#'@title Internal function to extract mutual correlation data.
#'
#'@description Computes and extracts pairwise correlation between the features
#'  used in a `familiarEnsemble` object.
#'
#'@inheritParams extract_data
#'
#'@return A list with a data.table containing mutual correlation between
#'  features.
#'@md
#'@keywords internal
setGeneric("extract_mutual_correlation", function(object,
                                                  data,
                                                  feature_cluster_method=waiver(),
                                                  feature_linkage_method=waiver(),
                                                  feature_similarity_metric=waiver(),
                                                  verbose=FALSE,
                                                  ...) standardGeneric("extract_mutual_correlation"))


#####extract_mutual_correlation#####
setMethod("extract_mutual_correlation", signature(object="familiarEnsemble", data="ANY"),
          function(object,
                   data,
                   feature_similarity_table,
                   feature_cluster_method=waiver(),
                   feature_linkage_method=waiver(),
                   feature_similarity_metric=waiver(),
                   verbose=FALSE){
            # Assess mutual correlation
            
            # Message extraction start
            if(verbose){
              logger.message(paste0("\tComputing similarity between important features."))
            }

            # Obtain cluster method from stored settings, if required.
            if(is.waive(feature_cluster_method)){
              feature_cluster_method <- object@settings$feature_cluster_method
            }
            
            # Obtain linkage function from stored settings, if required.
            if(is.waive(feature_linkage_method)){
              feature_linkage_method <- object@settings$feature_linkage_method
            }
  
            # Obtain feature similarity metric from stored settings, if required.
            if(is.waive(feature_similarity_metric)){
              feature_similarity_metric <- object@settings$feature_similarity_metric
            }
            
            # Replace feature cluster method == "none" with "hclust"
            if(feature_cluster_method == "none"){
              feature_cluster_method <- "hclust"
            }
            
            .check_cluster_parameters(cluster_method=feature_cluster_method,
                                      cluster_linkage=feature_linkage_method,
                                      cluster_similarity_metric=feature_similarity_metric,
                                      var_type="feature")
            
            # Check if any mutual correlation data are available.
            if(is_empty(feature_similarity_table)){
              return(list("data"=NULL,
                          "feature_similarity_metric"=feature_similarity_metric))
            }
            
            # Get the distance matrix from the feature_similarity_table
            distance_matrix <- cluster.get_distance_matrix(similarity_table=feature_similarity_table,
                                                           similarity_metric=feature_similarity_metric)
            
            # Obtain cluster object.
            h <- cluster.get_cluster_object(distance_matrix=distance_matrix,
                                            cluster_method=feature_cluster_method,
                                            cluster_linkage=feature_linkage_method)
            
            # Get data.table with feature ordering
            feature_order_table <- cluster.extract_label_order(cluster_object=h, cluster_method=feature_cluster_method)
            
            # Merge ordering into feature_similarity_table. The table is first
            # merged on feature_1 and then on feature_2.
            mutual_correlation_table <- data.table::copy(feature_similarity_table)
            mutual_correlation_table <- merge(x=mutual_correlation_table, y=feature_order_table,
                                              by.x="feature_1", by.y="name", all.x=TRUE, all.y=FALSE)
            mutual_correlation_table <- merge(x=mutual_correlation_table, y=feature_order_table,
                                              by.x="feature_2", by.y="name", all.x=TRUE, all.y=FALSE)
            
            # Rename columns
            data.table::setnames(mutual_correlation_table,
                                 old=c("label_order.x", "label_order.y"),
                                 new=c("label_order_1", "label_order_2"))
            
            # Reorder columns
            data.table::setcolorder(mutual_correlation_table,
                                    c("feature_1", "feature_2", "value", "label_order_1", "label_order_2"))
            
            # Add the name of the ensemble model
            mutual_correlation_table <- add_model_name(data=mutual_correlation_table, object=object)

            return(list("data"=mutual_correlation_table,
                        "feature_similarity_metric"=feature_similarity_metric,
                        "feature_cluster_object"=h))
          })


#'@title Internal function to extract feature expressions.
#'
#'@description Computes and extracts feature expressions for features
#'  used in a `familiarEnsemble` object.
#'
#'@param feature_similarity_table Table containing pairwise distance between
#'  sample. This is used to determine cluster information, and indicate which
#'  samples are similar. The table is created by the
#'  `extract_sample_similarity_table` method.
#'@inheritParams extract_data
#'@inheritParams extract_model_vimp
#'
#'@return A list with a data.table containing feature expressions.
#'@md
#'@keywords internal
setGeneric("extract_feature_expression", function(object,
                                                  data,
                                                  feature_similarity_table,
                                                  sample_similarity_table,
                                                  feature_cluster_method=waiver(),
                                                  feature_linkage_method=waiver(),
                                                  feature_similarity_metric=waiver(),
                                                  sample_cluster_method=waiver(),
                                                  sample_linkage_method=waiver(),
                                                  sample_similarity_metric=waiver(),
                                                  eval_times=waiver(),
                                                  verbose=FALSE,
                                                  ...) standardGeneric("extract_feature_expression"))

#####extract_feature_expression#####
setMethod("extract_feature_expression", signature(object="familiarEnsemble", data="ANY"),
          function(object,
                   data,
                   feature_similarity_table,
                   sample_similarity_table,
                   feature_cluster_method=waiver(),
                   feature_linkage_method=waiver(),
                   feature_similarity_metric=waiver(),
                   sample_cluster_method=waiver(),
                   sample_linkage_method=waiver(),
                   sample_similarity_metric=waiver(),
                   eval_times=waiver(),
                   verbose=FALSE){
            
            # Message extraction start
            if(verbose){
              logger.message(paste0("\tComputing sample clustering using important features."))
            }
            
            # Obtain feature cluster method from stored settings, if required.
            if(is.waive(feature_cluster_method)){
              feature_cluster_method <- object@settings$feature_cluster_method
            }
            
            # Obtain feature linkage function from stored settings, if required.
            if(is.waive(feature_linkage_method)){
              feature_linkage_method <- object@settings$feature_linkage_method
            }
            
            # Obtain feature similarity metric from stored settings, if required.
            if(is.waive(feature_similarity_metric)){
              feature_similarity_metric <- object@settings$feature_similarity_metric
            }
            
            # Replace feature cluster method == "none" with "hclust"
            if(feature_cluster_method == "none"){
              feature_cluster_method <- "hclust"
            }
            
            .check_cluster_parameters(cluster_method=feature_cluster_method,
                                      cluster_linkage=feature_linkage_method,
                                      cluster_similarity_metric=feature_similarity_metric,
                                      var_type="feature")
            
            # Obtain sample cluster method from stored settings, if required.
            if(is.waive(sample_cluster_method)){
              sample_cluster_method <- object@settings$sample_cluster_method
            }
            
            # Obtain sample linkage function from stored settings, if required.
            if(is.waive(sample_linkage_method)){
              sample_linkage_method <- object@settings$sample_linkage_method
            }
            
            # Obtain sample similarity metric from stored settings, if required.
            if(is.waive(sample_similarity_metric)){
              sample_similarity_metric <- object@settings$sample_similarity_metric
            }
            
            # Replace sample cluster method == "none" with "hclust"
            if(sample_cluster_method == "none"){
              sample_cluster_method <- "hclust"
            }
            
            .check_cluster_parameters(cluster_method=sample_cluster_method,
                                      cluster_linkage=sample_linkage_method,
                                      cluster_similarity_metric=sample_similarity_metric,
                                      var_type="sample")

            # Obtain evaluation times from the data.
            if(is.waive(eval_times) & object@outcome_type %in% c("survival", "competing_risk")){
              eval_times <- object@settings$eval_times
              
            } else if(is.waive(eval_times)){
              eval_times <- NULL
            }
            
            # Check if eval_times is correct.
            if(object@outcome_type %in% c("survival", "competing_risk")){
              sapply(eval_times, .check_number_in_valid_range, var_name="eval_times", range=c(0.0, Inf), closed=c(FALSE, TRUE))
            }
            
            # Aggregate data
            data <- aggregate_data(data=data)
            
            # Retrieve input data. Note that we batch_normalisation is taken
            # into account even though the data are converted back to their
            # original scale to derive expressions.
            data <- process_input_data(object=object, data=data, stop_at="batch_normalisation")
            
            if(is_empty(data)){
              return(NULL)
            }
            
            # Determine signature features
            model_features <- object@important_features
            
            # Maintain only important features. The current set is based on the
            # important features of the model, i.e. those that end up in the
            # model (potentially as a cluster).
            expression_data <- filter_features(data=data, available_features=model_features)
            
            # Perform inverse normalisation
            expression_data <- normalise_features(data=expression_data,
                                                  feature_info_list=object@feature_info,
                                                  invert=TRUE)
            
            # Perform inverse transformation
            expression_data <- transform_features(data=expression_data,
                                                  feature_info_list=object@feature_info,
                                                  invert=TRUE)
            
            # Extract expression data.
            expression_data <- expression_data@data
            
            # Extract feature info list.
            feature_info_list <- object@feature_info[model_features]
            
            # Check if any data are available for ordering features.
            if(!is_empty(feature_similarity_table)){
            
              # Get the distance matrix from the feature_similarity_table
              feature_distance_matrix <- cluster.get_distance_matrix(similarity_table=feature_similarity_table,
                                                                     similarity_metric=feature_similarity_metric)
              
              # Obtain cluster object that identifies similar features.
              h_feature <- cluster.get_cluster_object(distance_matrix=feature_distance_matrix,
                                                      cluster_method=feature_cluster_method,
                                                      cluster_linkage=feature_linkage_method)
              
              # Get data.table with feature ordered according to similarity.
              feature_order_table <- cluster.extract_label_order(cluster_object=h_feature,
                                                                 cluster_method=feature_cluster_method)
            } else {
              # Replacement for empty feature similarity tables
              feature_order_table <- data.table::data.table("name"=object@important_features)
              feature_order_table[, "label_order":=.I]
              
              # Placeholder cluster object
              h_feature <- NULL
            }
            
            # Check if any data are available for ordering samples.
            if(!is_empty(sample_similarity_table)){
            
              # Get the distance matrix from the sample_similarity_table.
              sample_distance_matrix <- cluster.get_distance_matrix(similarity_table=sample_similarity_table,
                                                                    similarity_metric=sample_similarity_metric)
              
              # Obtain cluster object that identifies similar samples.
              h_sample <- cluster.get_cluster_object(distance_matrix=sample_distance_matrix,
                                                     cluster_method=sample_cluster_method,
                                                     cluster_linkage=sample_linkage_method)
              
              # Get data.table with samples ordered according to similarity.
              sample_order_table <- cluster.extract_label_order(cluster_object=h_sample,
                                                                cluster_method=sample_cluster_method)
            } else {
              # Replacement for empty sample similarity tables
              sample_order_table <- data.table::data.table("name"=expression_data$subject_id)
              sample_order_table[, "label_order":=.I]
              
              # Placeholder cluster object
              h_sample <- NULL
            }
            
            # Add model name.
            expression_data <- add_model_name(data=expression_data, object=object)
            
            # Store to list. Note that the normalisation parameters are the
            # general normalisation parameter, not batch-normalisation parameters.
            expression_info <- list("expression_data"=expression_data,
                                    "feature_info"=feature_info_list,
                                    "feature_similarity_metric"=feature_similarity_metric,
                                    "feature_cluster_object"=h_feature,
                                    "feature_order"=feature_order_table,
                                    "sample_similarity_metric"=sample_similarity_metric,
                                    "sample_cluster_object"=h_sample,
                                    "sample_order"=sample_order_table,
                                    "evaluation_times"=eval_times)
            
            return(expression_info)
          })



#'@title Internal function to extract the confusion matrix.
#'
#'@description Computes and extracts the confusion matrix for predicted and
#'  observed categorical outcomes used in a `familiarEnsemble` object.
#'
#'@inheritParams extract_data
#'
#'@return A data.table containing predicted and observed outcome data together
#'  with a co-occurence count.
#'@md
#'@keywords internal
setGeneric("extract_confusion_matrix", function(object,
                                                prediction_data,
                                                verbose=FALSE,
                                                ...) standardGeneric("extract_confusion_matrix"))
  
#####extract_confusion_matrix#####
setMethod("extract_confusion_matrix", signature(object="familiarEnsemble", data="ANY"),
          function(object,
                   prediction_data,
                   verbose=FALSE){
            browser()
          })



#'@title Internal function to extract the feature distance table.
#'
#'@description Computes and extracts the feature distance table for features
#'  used in a `familiarEnsemble` object. This table can be used to cluster
#'  features, and is exported directly by `extract_mutual_correlation`.
#'
#'@inheritParams extract_data
#'
#'@return A data.table containing pairwise distance between features. This data
#'  is only the upper triangular of the complete matrix (i.e. the sparse
#'  unitriangular representation). Diagonals will always be 0.0 and the lower
#'  triangular is mirrored.
#'@md
#'@keywords internal
setGeneric("extract_feature_similarity_table", function(object,
                                                        data,
                                                        feature_similarity_metric=waiver(),
                                                        verbose=FALSE,
                                                        ...) standardGeneric("extract_feature_similarity_table"))

#####extract_feature_similarity_table#####
setMethod("extract_feature_similarity_table", signature(object="familiarEnsemble", data="ANY"),
          function(object,
                   data,
                   feature_similarity_metric=waiver(),
                   verbose=FALSE){
            
            # Message extraction start
            if(verbose){
              logger.message(paste0("\tComputing pairwise similarity for important features."))
            }
            
            # Obtain similarity metric from stored settings, if required.
            if(is.waive(feature_similarity_metric)){
              feature_similarity_metric <- object@settings$feature_similarity_metric
            }
            
            # Check correlation method
            .check_parameter_value_is_valid(x=feature_similarity_metric, var_name="feature_similarity_metric",
                                            values=.get_available_similarity_metrics(data_type="feature"))
            
            # Define an empty table prototype
            empty_similarity_table <- data.table::data.table("feature_1"=character(0),
                                                           "feature_2"=character(0),
                                                           "value"=numeric(0))
            
            # Retrieve input data.
            data <- process_input_data(object=object, data=data, stop_at="imputation")
            
            # Check if the input data is not empty
            if(is_empty(data)){
              return(empty_similarity_table)
            }
            
            # Check if the number of samples is sufficient (>5), and return an
            # empty table if not.
            if(data.table::uniqueN(data@data, by="subject_id") <= 5){
              return(empty_similarity_table)
            }
            
            # Maintain only important features. The current set is based on the
            # required features.
            data <- filter_features(data=data, available_features=object@important_features)
            
            # Identify eligble, numeric columns
            feature_columns <- get_feature_columns(x=data)
            
            # Break if there are not at least 2 features present between which
            # correlation can be compared.
            if(length(feature_columns) < 2) {
              return(empty_similarity_table)
            }
            
            # Compute the similarity table
            feature_similarity_table <- cluster.get_featurewise_similarity_table(data_obj=data,
                                                                                 feature_columns=feature_columns,
                                                                                 similarity_metric=feature_similarity_metric,
                                                                                 verbose=FALSE)
            
            return(feature_similarity_table)
          })



#'@title Internal function to extract the sample distance table.
#'
#'@description Computes and extracts the sample distance table for samples
#'  analysed using a `familiarEnsemble` object to form a `familiarData` object. This table can be used to cluster
#'  samples, and is exported directly by `extract_feature_expression`.
#'
#'@inheritParams extract_data
#'
#'@return A data.table containing pairwise distance between samples. This data
#'  is only the upper triangular of the complete matrix (i.e. the sparse
#'  unitriangular representation). Diagonals will always be 0.0 and the lower
#'  triangular is mirrored.
#'@md
#'@keywords internal
setGeneric("extract_sample_similarity_table", function(object,
                                                       data,
                                                       sample_similarity_metric=waiver(),
                                                       verbose=FALSE,
                                                       ...) standardGeneric("extract_sample_similarity_table"))

#####extract_sample_similarity_table#####
setMethod("extract_sample_similarity_table", signature(object="familiarEnsemble", data="ANY"),
          function(object,
                   data,
                   sample_similarity_metric=waiver(),
                   verbose=FALSE){
            
            # Message extraction start
            if(verbose){
              logger.message(paste0("\tComputing pairwise similarity between samples."))
            }
            
            # Obtain similarity metric from stored settings, if required.
            if(is.waive(sample_similarity_metric)){
              sample_similarity_metric <- object@settings$sample_similarity_metric
            }
            
            # Check correlation method
            .check_parameter_value_is_valid(x=sample_similarity_metric, var_name="sample_similarity_metric",
                                            values=.get_available_similarity_metrics(data_type="sample"))
            
            # Define an empty table prototype
            empty_similarity_table <- data.table::data.table("sample_1"=character(0),
                                                             "sample_2"=character(0),
                                                             "value"=numeric(0))
            
            # Retrieve input data.
            data <- process_input_data(object=object, data=data, stop_at="imputation")
            
            # Check if the input data is not empty
            if(is_empty(data)){
              return(empty_similarity_table)
            }
            
            # Check if the number of samples is sufficient to form pairs (>= 2),
            # and return an empty table if not.
            if(data.table::uniqueN(data@data, by="subject_id") < 2){
              return(empty_similarity_table)
            }
            
            # Maintain only important features. The current set is based on the
            # required features.
            data <- filter_features(data=data, available_features=object@important_features)
            
            # Aggregate features.
            data <- aggregate_data(data=data)
            
            # Identify eligble, numeric columns
            feature_columns <- get_feature_columns(x=data)
            
            # Break if there are is not at least 1 feature present between which
            # similarity can be compared between samples.
            if(length(feature_columns) < 2) {
              return(empty_similarity_table)
            }
            
            # Compute the similarity table
            sample_similarity_table <- cluster.get_samplewise_similarity_table(data_obj=data,
                                                                                similarity_metric=sample_similarity_metric,
                                                                                verbose=FALSE)
            
            return(sample_similarity_table)
          })



.process_single_iter_performance <- function(prediction_table, metric, learner, outcome_type, samples=NULL){
  # Low level function calculate discrimination performance scores
  
  if(nrow(prediction_table) == 0){
    # Generate dummy output.
    # This also captures length(samples) == 0
    dummy_list <- list()
    for(metric_col_name in paste0("performance_", metric)){
      dummy_list[[metric_col_name]] <- 1.0
    }
    return(head(data.table::data.table(dummy_list), n=0))
  }
  
  # Generate a subsample of the prediction table if required
  if(!is.null(samples)){
    sample_table     <- data.table::data.table("subject_id"=samples)
    prediction_table <- merge(sample_table, prediction_table, all.x=TRUE, all.y=FALSE, allow.cartesian=TRUE)
  }
  
  # Create place-holder list
  performance_list   <- list()
  
  # Iterate over performance metrics
  for(current_metric in metric){
    # Parse metric to column name
    metric_col_name  <- paste0("performance_", current_metric)
    
    # Extract performance metric
    performance_list[[metric_col_name]] <- metric.main(metric=current_metric, learner=learner, purpose="score", dt=prediction_table, outcome_type=outcome_type, na.rm=TRUE)
  }
  
  # Parse list of performances to a data.table and return
  return(data.table::as.data.table(performance_list))
}



.prepare_basic_auc_table <- function(current_class, prediction_table, outcome_type){
  # Prepares a basic AUC table that is sorted by probability of the positive class and has matching outcomes identified
  
  # Suppress NOTES due to non-standard evaluation in data.table
  pos_class_probability <- outcome <- NULL
  
  if(is_empty(prediction_table)){
    # If input data is lacking, the table cannot be created.
    return(data.table::data.table("subject_id"=character(0),
                                  "pos_class"=character(0),
                                  "is_pos_class"=logical(0),
                                  "pos_class_probability"=numeric(0)))
  }
  
  # Determine the name of the probability column for the current class
  current_class_col <- getClassProbabilityColumns(outcome_type=outcome_type, class_levels=current_class)
  
  # Extract the required data
  data <- data.table::copy(prediction_table[, c("subject_id", "outcome", current_class_col), with=FALSE])
  
  # Rename the "current_class_col" column to a more general name.
  data.table::setnames(data, current_class_col, "pos_class_probability")
  
  # Only keep data with valid probabilities
  data <- data[is.finite(pos_class_probability)]
  
  if(is_empty(data)){
    # If input data is lacking, the table cannot be created.
    return(data.table::data.table("subject_id"=character(0),
                                  "pos_class"=character(0),
                                  "is_pos_class"=logical(0),
                                  "pos_class_probability"=numeric(0)))
  }
  
  # Order with decreasing class probability
  data <- data[order(-pos_class_probability)]
  
  # Find entries that match the expected class
  data[, "is_pos_class":=outcome == current_class]
  
  # Drop outcome and probability and add the positive class
  data[, ":="("outcome"=NULL, "pos_class"=current_class)]
  
  # Reorder columns
  data.table::setcolorder(data, neworder=c("subject_id", "pos_class", "is_pos_class", "pos_class_probability"))
  
  return(data)
}



.process_auc_table <- function(basic_auc_table, samples=NULL){
  # Low level function to compute the AUC table
  
  # Suppress NOTES due to non-standard evaluation in data.table
  y <- pos_class_probability <- is_pos_class <- x <- NULL
  
  if(nrow(basic_auc_table) == 0){
    return(data.table::data.table("x"=numeric(0), "y"=numeric(0)))
  }
  
  if(is.null(samples)){
    data <- data.table::copy(basic_auc_table)
  } else {
    sample_table     <- data.table::data.table("subject_id"=samples)
    data <- merge(basic_auc_table, sample_table, by="subject_id", all.x=FALSE, all.y=TRUE, allow.cartesian=TRUE)[order(-pos_class_probability)]
  }
  
  # Add sensititivity (y) and 1-specificity (x)
  data[, ":="(x=cumsum(!is_pos_class), "y"=cumsum(is_pos_class))]
  
  # Determine the number of positive classes (n_y) and negative classes (n_x)
  n_x <- tail(data, n=1)$x
  n_y <- tail(data, n=1)$y
  
  if(n_x==0 | n_y==0){
    # A table cannot be drawn if there are only positive or negative classes.
    return(data.table::data.table("x"=numeric(0), "y"=numeric(0)))
  }
  
  # Keep only unique points in x
  data <- data[, list(y=max(y)), by="x"]
  
  # Update x and y
  data[, ":="("x"=x/n_x, "y"=y/n_y)]
  
  return(data)
}



.complete_auc_table <- function(basic_auc_table, compute_confidence_intervals=FALSE, conf_alpha=0.05){
  
  # Suppress NOTES due to non-standard evaluation in data.table
  y <- NULL
  
  if(is_empty(basic_auc_table)){
    # Catch cases where the input is invalid.
    if(!compute_confidence_intervals){
      return(data.table::data.table("pos_class"=character(0), "x"=numeric(0), "y"=numeric(0)))
      
    } else {
      return(data.table::data.table("pos_class"=character(0), "x"=numeric(0), "y"=numeric(0), "conf_int_lower"=numeric(0), "conf_int_upper"=numeric(0)))
    }
  }
  
  # Determine the positive class
  current_pos_class <- head(basic_auc_table, n=1)$pos_class
  
  if(!compute_confidence_intervals){
    # Calculate the auc_table directly from the data.
    auc_table <- .process_auc_table(basic_auc_table=basic_auc_table, samples=NULL)
    
    if(nrow(auc_table) > 0){
      # Add in initial data. Endpoints are always present in vallid auc_tables.
      auc_table <- rbind(data.table::data.table("x"=0.0, "y"=0.0), auc_table)
      
      # Add positive class column and reorder columns
      auc_table[, "pos_class":=current_pos_class]
      data.table::setcolorder(auc_table, "pos_class")
    } else {
      auc_table <- data.table::data.table("pos_class"=character(0), "x"=numeric(0), "y"=numeric(0))
    }
    
    return(auc_table)
    
  } else {
    # Compute confidence intervals for the AUC table
    n_iter <- ceiling(20 / conf_alpha)
    
    auc_table <- data.table::rbindlist(lapply(seq_len(n_iter), function(ii, basic_auc_table){
      # Create samples
      samples <- sample(x=basic_auc_table$subject_id, size=nrow(basic_auc_table), replace=TRUE)
      
      # Generate the AUC table
      auc_table <- .process_auc_table(basic_auc_table=basic_auc_table, samples=samples)
      
      # Check cases
      if(nrow(auc_table) == 0){
        auc_table <- data.table::data.table("x"=c(0.0, 1.0), "y"=c(0.0, 1.0))
      } else {
        if(head(auc_table, n=1)$x > 0){
          auc_table <- rbind(data.table::data.table("x"=0.0, "y"=0.0), auc_table)
        }
      }
      
      # Approximate function at small intervals
      x_out <- seq(from=0.0, to=1.0, by=0.01)
      
      auc_table <- data.table::data.table("x"=x_out,
                                          "y"=stats::approx(x=auc_table$x, y=auc_table$y, xout=x_out, method="linear", rule=2)$y)
      
      return(auc_table)
      
    }, basic_auc_table=basic_auc_table))
    
    # Aggregate results
    auc_table <- auc_table[, list(y=stats::median(y), conf_int_lower=stats::quantile(y, conf_alpha/2),
                                  conf_int_upper=stats::quantile(y, 1-conf_alpha/2)), by="x"]
    
    # Add in initial data point. Endpoints are always present in vallid auc_tables.
    auc_table_initial <- head(auc_table, n=1)
    auc_table_initial[, ":="("x"=0.0, "y"=0.0)]
    
    # Combine.
    auc_table <- rbind(auc_table_initial, auc_table)
    
    # Add positive class column and reorder columns
    auc_table[, "pos_class":=current_pos_class]
    data.table::setcolorder(auc_table, "pos_class")
    
    return(auc_table)
  }
}
