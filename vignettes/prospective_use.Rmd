---
title: "Using familiar prospectively"
author: "Alex Zwanenburg"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    includes:
      after_body: license.html
    toc: TRUE
bibliography: "refs.bib"
vignette: >
  %\VignetteIndexEntry{Using familiar prospectively}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.width = 7
)
```

```{r setup, results=FALSE}
library(familiar)
library(data.table)

set.seed(19)
```

The `summon_familiar` function is used to generate models for a given dataset and provide a broad analysis afterwards. Many of the files resulting from this analysis can also be used outside of `summon_familiar`. For example models and ensembles can be used prospectively to assess new datasets. Likewise data and collection objects can be used to customise plotting and export to tables.

In this example we will use the Iris dataset, which we first randomly split into development and validation datasets:

```{r, results=FALSE}
# Load Iris data set
data <- data.table::as.data.table(iris)

# Add sample and cohort id
data[,":="("sample_id"=.I)]

# Generate training and validation samples
train_samples <- sample(data$sample_id, size=100, replace=FALSE)
valid_samples <- setdiff(data$sample_id, train_samples)

# Assign batch identifiers.
data[sample_id %in% train_samples, "batch_id":="development"]
data[sample_id %in% valid_samples, "batch_id":="validation"]
```

Then we summon our familiar to create models for the data, and assess it. We will create five multinomial logistic regression models to predict the iris species, based on bootstraps of the development dataset. You may notice that we here write to the temporary R directory using `tempdir()`. In practice you will want to use a different directory, as the temporary R directory will be removed once your R session closes. For speed, we will also only compute point estimates during evaluation.

```{r, results="hide", message=FALSE, warning=FALSE}
familiar::summon_familiar(data=data,
                          project_dir=tempdir(),
                          sample_id_column="sample_id",
                          batch_id_column="batch_id",
                          development_batch_id="development",
                          outcome_type="multinomial",
                          outcome_column="Species",
                          experimental_design="bt(fs+mb,5) + ev",
                          cluster_method="none",
                          fs_method="mrmr",
                          learner="glm",
                          parallel=FALSE,
                          estimation_type="point")
```

# Using models prospectively

Models generated by familiar are stored in subdirectories of the `trained_models` folder:

```{r}
# Create path to the directory containing the models.
model_directory_path <- file.path(tempdir(), "trained_models", "glm", "mrmr")

# List files present in the directory.
list.files(model_directory_path)
```

There are 5 models in the directory, which are stored in RDS format in files ending with `*_model.RDS`. We can inspect the first model in the directory.

```{r}
# Create path to the model.
model_path <- file.path(model_directory_path, list.files(model_directory_path, pattern="model")[1])

# Load the model.
model <- readRDS(model_path)
model
```

This model can then be used to predict values for a given dataset, among other things:

```{r}
# To be implemented.
```

More powerful however, is the ability to perform any of the evaluation and explanation steps for a new dataset, including new settings. By default all evaluation and explanation steps are conducted using the settings defined when running `summon_familiar`. In this case that means that point estimates will be computed.

Let us for example compute and plot model performance AUC-ROC and accuracy for the model.

```{r, fig.align='center', fig.width=14}
plots <- familiar::plot_model_performance(object=model,
                                          draw=TRUE,
                                          facet_by="metric",
                                          data=data[batch_id=="validation"],
                                          metric=c("auc", "accuracy"))
```

You may notice that no plot is produced. This is because the type of plot `violin_plot` and the `estimation_type` inherited from the model are incompatible. However, nothing prevents us from changing the estimation type to bootstrap confidence intervals `bci`.

```{r}
# Draw model performance plots with bootstrap confidence intervals. familiar_data_names argument specifies the name that appears below the plot. The default is rather long.
plots <- familiar::plot_model_performance(object=model,
                                          draw=TRUE,
                                          facet_by="metric",
                                          data=data[batch_id=="validation"],
                                          estimation_type="bci",
                                          metric=c("auc", "accuracy"),
                                          familiar_data_names="GLM")
```

Note that we can achieve the same result without explicitly importing the model. Providing the path to the model as the `object` argument suffices:

```{r}
plots <- familiar::plot_model_performance(object=model_path,
                                          draw=TRUE,
                                          facet_by="metric",
                                          data=data[batch_id=="validation"],
                                          estimation_type="bci",
                                          metric=c("auc", "accuracy"),
                                          familiar_data_names="GLM")
```

# Using ensembles of models prospectively

The five models created in the example form an ensemble. Instead of investigating the models separately, we can also the model ensemble. Model ensembles generated by familiar are stored in the same subdirectory of the `trained_models` folder as their constituent models:

```{r}
# List files present in the directory.
list.files(model_directory_path)
```

In this case there is only one ensemble in the directory, which is stored in RDS format in a file ending with `*_ensemble.RDS`. Some alternative experiment designs, e.g. ones involving cross-validation, can lead to multiple ensembles being formed. We can inspect the ensemble in the directory.

```{r}
# Create path to the model.
ensemble_path <- file.path(model_directory_path, list.files(model_directory_path, pattern="ensemble")[1])

# Load the model.
ensemble <- readRDS(ensemble_path)
ensemble
```

Ensembles behave similarly to models during evaluation and explanation steps:

```{r}
plots <- familiar::plot_model_performance(object=ensemble,
                                          draw=TRUE,
                                          facet_by="metric",
                                          data=data[batch_id=="validation"],
                                          estimation_type="bci",
                                          metric=c("auc", "accuracy"),
                                          familiar_data_names="GLM")
```

There is one important caveat. Normally, when loading an ensemble, the models are not attached to the ensemble. Instead the `model_list` attribute of the ensemble object contains a list of paths to the location of the model files at creation. Thus, if you move these files, the ensemble can no longer find and attach the models. There are two ways to avoid this issue.

The first way is to use the `update_model_dir_path` method to point the ensemble to the new directory. The second, more generic way, is to create an ensemble on the fly from the underlying models. To do so, we supply the `object` argument in export and plot methods with a list of models, or paths to these models.

```{r}
# Generate paths to the model objects.
model_paths <- sapply(list.files(model_directory_path, pattern="model"), function(x) (file.path(model_directory_path, x)))

# Generate plot using an ad-hoc ensemble.
plots <- familiar::plot_model_performance(object=model_paths,
                                          draw=TRUE,
                                          facet_by="metric",
                                          data=data[batch_id=="validation"],
                                          estimation_type="bci",
                                          metric=c("auc", "accuracy"),
                                          familiar_data_names="GLM")
```

# Customising exports and plots

Familiar also produces data and collection objects. Data objects contain the processed evaluation data derived from a particular data set. These are located in the `familiar_data` folder.

```{r}
list.files(file.path(tempdir(), "familiar_data"))
```

In our example, we generated two data objects: one for development and one for validation data. These separate data objects are collected in a collection object, located in the `familiar_collections` folder.

```{r}
list.files(file.path(tempdir(), "familiar_collections"))
```

There is typically only one collection in this location, but more may exist if `summon_familiar` is called with `evaluate_top_level_only=FALSE`.

Note that data and collection objects are actually static. We cannot use them as flexibly as models and ensembles. For example, we cannot assess different performance metrics using the data stored in familiar data and collection objects, or use these objects to predict outcomes for new datasets. There are some exceptions to this rule:

-   `export_fs_vimp`, `export_model_vimp`, and `plot_variable_importance` and its derived methods (`plot_feature_selection_variable_importance`, `plot_feature_selection_occurrence`, `plot_model_signature_variable_importance` and `plot_model_signature_occurrence`) allow for specifying and altering feature aggregation methods and thresholds.

-   `export_feature_similarity`, `export_sample_similarity`, `export_feature_expression`, `plot_feature_similarity` and `plot_sample_clustering` allow for specifying clustering arguments, but not the similarity metric to assess distance between features. Internally, data and collection objects store distance matrices.

The primary use of data and collection objects is for customising plotting. For example, the AUC-ROC curves for each species are plotted using the default palette in familiar, and a custom theme based on `cowplot::theme_cowplot`. We can re-create the plot using the standard R palette, and a different theme to alter its appearance.

```{r}
collection <- file.path(tempdir(), "familiar_collections", "pooled_data.RDS")

plots <- plot_auc_roc_curve(object=collection,
                            ggtheme=ggplot2::theme_dark(),
                            discrete_palette="R4",
                            draw=TRUE)
```
