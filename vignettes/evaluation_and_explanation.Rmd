---
title: "Evaluation and explanation"
author: "Alex Zwanenburg"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    includes:
      after_body: license.html
    toc: TRUE
bibliography: "refs.bib"
vignette: >
  %\VignetteIndexEntry{Introducing familiar}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(familiar)
library(data.table)
```

Familiar will automatically evaluate created models, and export corresponding tables and plots. The different analyses are shown in the table below. Several analyses allow for determining distributions and confidence intervals to provide an indication of the value spread for a given dataset and models (if applicable). What kind of estimates are computed is defined by the `estimation_type` argument:

-   `point`: Point estimates, i.e. single values.

-   `bias_correction` or `bc`: Bias-corrected estimates. A bias-corrected estimate is computed from (at least) 20 point estimates.

-   `bootstrap_confidence_interval` or `bci`: Bias-corrected estimates with bootstrap confidence intervals [@Efron2016-ws]. This estimation type allows for plotting confidence intervals in plots. The number of point estimates required depends on the `confidence_level` parameter. Familiar uses a rule of thumb of $n=20/(1-\textrm{confidence_level})$, i.e. at least 400 points for `confidence_level=0.95`.

Another argument, `detail_level` determines how the required point estimates are formed:

-   `ensemble`: Point estimates are computed at the ensemble level, i.e. over all models in the ensemble. This means that, for example, bias-corrected estimates of model performance are assessed by creating (at least) 20 bootstraps and computing the model performance of the ensemble model for each bootstrap.

-   `hybrid`: Point estimated are computed at from the models in an ensemble. This means that, for example, bias-corrected estimates of model performance are directly computed using the models in the ensemble. If there are at least 20 trained models in the ensemble, performance is computed for each model, in contrast to `ensemble` where performance is computed for the ensemble of models. If there are less than 20 trained models in the ensemble, bootstraps are created so that at least 20 point estimates can be made.

-   `model`: Point estimates are computed at the model level. This means that, for example, bias-corrected estimates of model performance are assessed by creating (at least) 20 bootstraps and computing the performance of the model for each bootstrap. Plots will not be automatically created in this case.

Generally, the more point estimates are required, the more computationally intensive the analysis is. Some analyses also become computationally more expensive with more samples. The `sample_limit` argument can be used to specify the maximum number of samples that should be used.

Some analyses, such as model predictions and individual conditional expectation plots, do not benefit from bootstraps as they are directly based on values predicted by the models. To compute bias-corrected estimates or bootstrap confidence intervals for these analyses requires that sufficient models are created. Experimental designs such as `experiment_design="bs(fs+mb,400)+ev"` allow for this, but are computationally expensive.

| Name                                     | Plot function                                                                     | Export function                       | Estimation type | Detail level | Sample limit |
|------------------------------------------|-----------------------------------------------------------------------------------|---------------------------------------|:---------------:|:------------:|:------------:|
| AUC-PR^a^                                | `plot_auc_precision_recall_curve`                                                 | `export_auc_data`                     |        ×        |      ×       |              |
| AUC-ROC^a^                               | `plot_auc_roc_curve`                                                              | `export_auc_data`                     |        ×        |      ×       |              |
| Calibration info^c^                      |                                                                                   | `export_calibration_info`             |                 |      ×       |              |
| Confusion matrix^a^                      | `plot_confusion_matrix`                                                           | `export_confusion_matrix_data`        |                 |      ×       |              |
| Decision curve analysis^ab^              | `plot_decision_curve`                                                             | `export_decision_curve_analysis_data` |        ×        |      ×       |              |
| Feature expression^c^                    | `plot_sample_clustering`                                                          | `export_feature_expressions`          |                 |              |              |
| Feature selection variable importance^c^ | `plot_feature_selection_occurrence`; `plot_feature_selection_variable_importance` | `export_fs_vimp`                      |                 |              |              |
| Feature similarity^c^                    | `plot_feature_similarity`                                                         | `export_feature_similarity`           |        ×        |              |              |
| Hyperparameters^c^                       |                                                                                   | `export_hyperparameters`              |                 |              |              |
| Individual conditional expectation^c^    | `plot_ice`                                                                        | `export_ice_data`                     |      ×^d^       |      ×       |      ×       |
| Model calibration^ce^                    | `plot_calibration_data`                                                           | `export_calibration_data`             |        ×        |      ×       |              |
| Model performance^c^                     | `plot_model_performance`                                                          | `export_model_performance`            |        ×        |      ×       |              |
| Model predictions^c^                     |                                                                                   | `export_prediction_data`              |      ×^d^       |      ×       |              |
| Model variable importance^ce^            | `plot_model_signature_occurrence`; `plot_model_signature_variable_importance`     | `export_model_vimp`                   |                 |              |              |
| Partial dependence^c^                    | `plot_pd`                                                                         | `export_partial_dependence_data`      |      ×^d^       |      ×       |      ×       |
| Permutation variable importance^c^       | `plot_permutation_variable_importance`                                            | `export_permutation_vimp`             |        ×        |      ×       |              |
| Risk stratification info^b^              |                                                                                   | `export_risk_stratification_info`     |                 |      ×       |              |
| Risk stratification data^b^              | `plot_kaplan_meier`                                                               | `export_risk_stratification_data`     |                 |      ×       |              |
| Sample similarity^c^                     |                                                                                   | `export_sample_similarity`            |        ×        |              |      ×       |
| Univariate analysis^c^                   | `plot_univariate_importance`                                                      | `export_univariate_analysis_data`     |                 |              |              |

: Evaluation and explanation steps.\
^a^ Available for binomial and multinomial outcomes.\
^b^ Available for survival outcomes.\
^c^ Available for all outcomes.\
^d^ Estimation types other than `point` require sufficient models.\
^e^ May not be available for all models.

Plots and tables are exported when familiar is run with `summon_familiar`. The plot and export functions can all be used externally as well, which is described in the *Using familiar prospectively* vignette. Indeed, to showcase some of the functionality we will use the techniques discussed there. In practical use, the

We will first create two models. One is trained on (a subset of) the Wisconsin breast cancer dataset [@Wolberg1990-gn] to predict cell malignancy from biopsies, which constitutes a classification problem.

```{r, results=FALSE, eval=TRUE, message=FALSE, warning=FALSE}

# Get a dataset.
binomial_data <- familiar:::test.create_good_data_set(outcome_type="binomial")

# Call familiar to create models. Note that we skip hyperparameter optimisation by manually defining the hyperparameters, and that we skip evaluation steps. This is to save time when creating the vignette.
familiar::summon_familiar(data=binomial_data,
                          project_dir=file.path(tempdir()),
                          experiment_dir="binomial",
                          experimental_design="fs+mb",
                          cluster_method="none",
                          fs_method="none",
                          hyperparameter=list("random_forest_ranger"=
                                                list("n_tree"=4,
                                                     "sample_size"=0.7,
                                                     "m_try"=0.5,
                                                     "node_size"=20,
                                                     "tree_depth"=3)),
                          learner="random_forest_ranger",
                          parallel=FALSE,
                          skip_evaluation_elements="all")

# Create path to the directory containing the models.
model_directory_path <- file.path(tempdir(), "binomial", "trained_models", "random_forest_ranger", "none")
model_path <- file.path(model_directory_path, list.files(model_directory_path, pattern="model")[1])

# Load the model.
binomial_model <- readRDS(model_path)
```

The second model is a survival model, which is trained on (a subset of) a colon chemotherapy clinical trial dataset [@Moertel1995-bh].

```{r, results=FALSE, eval=TRUE, message=FALSE, warning=FALSE}

# Get a dataset.
survival_data <- familiar:::test.create_good_data_set(outcome_type="survival")

# Call familiar to create models. Note that we skip hyperparameter optimisation by manually defining the hyperparameters, and that we skip evaluation steps. This is to save time when creating the vignette.
familiar::summon_familiar(data=survival_data,
                          project_dir=file.path(tempdir()),
                          experiment_dir="survival",
                          experimental_design="fs+mb",
                          cluster_method="none",
                          fs_method="none",
                          hyperparameter=list("random_forest_ranger"=
                                                list("n_tree"=9,
                                                     "sample_size"=0.95,
                                                     "m_try"=0.68,
                                                     "node_size"=30,
                                                     "tree_depth"=2,
                                                     "alpha"=0.25)),
                          learner="random_forest_ranger",
                          parallel=FALSE,
                          skip_evaluation_elements="all")

# Create path to the directory containing the models.
model_directory_path <- file.path(tempdir(), "survival", "trained_models", "random_forest_ranger", "none")
model_path <- file.path(model_directory_path, list.files(model_directory_path, pattern="model")[1])

# Load the model.
survival_model <- readRDS(model_path)
```

# Model performance

Model performance is usually assessed to evaluate accuracy of a model.

The main assessment uses metrics to quantify model performance. These metrics are documented in the *Performance metrics* vignette, and can be set using the `evaluation_metric` argument of the `summon_familiar` function or `metric` for plot and export functions. For example, this results in the following plot for the breast cancer dataset:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_model_performance(object=binomial_model,
                                          data=binomial_data,
                                          metric = c("auc", "brier", "f1_score"),
                                          x_axis_by="metric")

plots[[1]]
```

The plot shows distributions of the area under the receiver operating characteristic curve, the brier score and the f1-score. The plot indicates that the model predicts accurately for the given dataset. This is to be expected, as the classification problem is quite easy, and we are evaluating the development dataset.

To compute model performance metric scores for a dataset, familiar first predicts response values for instances in a dataset. These response values are then compared against the observed outcome using the specified metric.

## Receiver-operating characteristic curve

The receiver-operating characteristic (ROC) curve visualises concordance between predicted class probabilities and the observed classes [@Hand2001-il]. It shows the trade-off between sensitivity and specificity of the model. For the breast cancer dataset the receiver-operating characteristic curve is as follows:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_auc_roc_curve(object=binomial_model,
                                      data=binomial_data)

plots[[1]]
```

Under ideal circumstances, the curve would lie in the top-left corner of the plot. If a model does not predict better than at random, the curve would lie along the diagonal, which corresponds to an AUC-ROC value of 0.5. The plot above indicates that the model is able to differentiate between benign and malignant cells in breast cancer biopsy.

An ROC curve is created by ascendingly sorting instances by the predicted probability of the positive class, and computing the number of true positive, true negative, false positive, false negative cases observed at each instance. These are then used to compute sensitivity and specificity.

The above plot shows the average of 400 single curves, with 95% confidence intervals. To achieve this, familiar interrogates each curve over the complete range of potential specificity values ($[0.000, 1.000]$) with a step size of $0.005$. Linear interpolation is used for this purpose [@Davis2006-xb]. This allows for assessing the distribution of sensitivity at fixed specificity values.

The above approach is the general approach used by familiar. An exact ROC curve can only be plotted when only the point estimate (`estimation_type="point"`) of a single model or the complete ensemble of models is evaluated (`detail_level="ensemble"`):

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_auc_roc_curve(object=binomial_model,
                                      data=binomial_data,
                                      estimation_type="point")

plots[[1]]
```

## Precision-recall curve

The precision-recall (PR) curve shows the trade-off between precision (positive predictive value) and recall (sensitivity). An important distinction between the PR curve and the ROC curve described above is that precision does not assess true negatives but false positives. In case the number of negative instances greatly exceeds the number of positive instances , a large change in the number of false positives reduces the ROC curve somewhat, but this change is far easier observed in the PR curve [@Davis2006-xb]. Hence the PR curve is commonly assessed if the positive class is more important than and relatively rare compared to a negative class. For the breast cancer dataset the precision-recall curve is as follows:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_auc_precision_recall_curve(object=binomial_model,
                                                   data=binomial_data)

plots[[1]]
```

Under ideal circumstances, the curve would lie in the top-right part of the plot. A random classifier would yield a curve that is mostly horizontal and located at the fraction of positive instances. This would equal a precision of 45% for the current dataset.

Like the ROC curve, the PRcurve is formed by ascendingly sorting instances by the predicted probability of the positive class, and computing the number of true positive and false positive negative cases observed at each instance. These are then used to compute recall and precision.

The above plot shows the average of 400 single curves, with 95% confidence intervals. To achieve this, familiar interrogates each curve over the complete range of recall values ($[0.000, 1.000]$) with a step size of $0.005$. We use linear interpolation for this purpose, treating the PR curve as a piecewise function to handle the situation identified by Davis and Goadrich [@Davis2006-xb]. Interpolating in this manner allows for assessing the distribution of precision values at fixed recall values.

Again, this is the general approach used by familiar. An exact PR curve is only plotted when evaluating the point estimate (`estimation_type="point"`) of a single model or of the complete ensemble of models (`detail_level="ensemble"`). In the breast cancer dataset, this produces the following curve:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_auc_precision_recall_curve(object=binomial_model,
                                                   data=binomial_data,
                                                   estimation_type="point")

plots[[1]]
```

## Confusion matrix

A confusion matrix schematically represents the number of predicted (expected) class instances and their actual (observed) values. This may help identify potential weaknesses of classifiers, i.e. false positives or negatives.

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_confusion_matrix(object=binomial_model,
                                         data=binomial_data)

plots[[1]]
```

Familiar selects the class with the highest predicted class probability as the expected class. It is currently not possible in familiar to obtain confusion matrices for set probability thresholds, such as the threshold that maximises the Matthews correlation coefficient.

## Kaplan-Meier survival curves

Kaplan-Meier survival curves are a standard plot to assess survival over time in a population, or survival differences between groups. In familiar, instances are stratified to risk groups by applying one or more thresholds. These thresholds are created during model development to avoid bias. The number and value of these thresholds is determined by two parameters: `stratification_method` and `stratification_threshold`. Both parameters are set when calling `summon_familiar`. `stratification_method` has three options:

-   `median`: The median predicted value in the development cohort is used to stratify instances into two risk groups.

-   `fixed`: Instances are stratified based on the sample quantiles of the predicted values. These quantiles are defined using the `stratification_threshold` parameter.

-   `optimised`: Use maximally selected rank statistics to determine the optimal threshold [@Lausen1992-qh; @Hothorn2003-qn] to stratify instances into two optimally separated risk groups.

The `stratification_threshold` parameter is only used when `stratification_method="fixed"`. It allows for specifying the sample quantiles that are used to determine the thresholds. For example `stratification_threshold=c(0.25,0.75)` creates two thresholds that stratifies the development dataset into three groups: one with 25% of the instances with the highest risk, a group with 50% of the instances with medium risk, and a third group with 25% of instances with lowest risk.

Applying the survival model to the colon cancer dataset yields the following Kaplan-Meier plot:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_kaplan_meier(object=survival_model,
                                     data=survival_data)

grid::grid.newpage()
grid::grid.draw(plots[[1]])
```

In the plot above, instances are divided into two groups by the median
threshold. The risk groups are shown with 95% confidence intervals. Familiar
uses the `survival::survfit` function to create both the survival curves and the
confidence intervals. The low risk group has significantly better survival than
the high risk group. This is also indicated in the bottom-left of the main plot,
which shows the result of a logrank test between the two groups. Censored
patients are indicated by crosses. The number of surviving patients in both
groups at the time points along the x-axis are shown below the main plot.

Note that `predict` method does allow for manually specifying stratification
thresholds for existing models.

# Model calibration

Model performance is an import aspect to evaluate because it tells us how
accurate it is. However, it often does not tell us how well we can rely on
predictions for individual instances, e.g. how accurate an instance class
probability is. Model calibration is assessed for this purpose. The model for
the breast cancer dataset produces the following calibration plot:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_calibration_data(object=binomial_model,
                                         data=binomial_data)

grid::grid.newpage()
grid::grid.draw(plots[[1]])
```

The calibration curve shows the expected (predicted) probability of malignant
cells versus the observed proportion of malignant cells, together with a 95%
confidence interval. In an ideal calibration plot the calibration curve should
lie on the diagonal (dashed) line. Here the upper part of the curve (expected
probability \> 0.50) follows the diagonal nicely. The lower part of the curve
dips below the diagonal, indicating that predicted probabilities in this range
tend to overestimate malignancy. However, the density plot in the top panel
indicates that predicted probabilities were generally close to 0.0 or 1.0, with
very few instances in between.

Familiar also performs several calibration tests to assess different levels of
calibration [@Van_Calster2016-ok]. First of all, we assess two aspects of the
linear fit (solid line), the intercept (calibration-in-the-large) and slope.
These are show in the top-left of the main plot.

The intercept ideally is 0.00. In our example, it lies below this point, and is
not contained in the 95% confidence interval. We therefore conclude that the
intercept is significantly different at the $\alpha=0.05$ level. That being
said, this difference may not be relevant in practice, as it is quite small.

In an ideal case, the slope of the linear fit is 1.00. In our example, the slope
of the linear fit lies above this value. This indicates that, overall, the model
tends to overestimate the probability of malignant cells, but not to a large
degree.

One issue with assessing calibration through a linear fit, is that the
calibration curve can be decidedly non-linear and still produce good values for
the intercept and slope. Therefore, calibration is also assessed using
statistical tests: the Hosmer-Lemeshow (HL) test for categorical and regression
outcomes [@Hosmer1997-ov], and the Nam-D'Agostino (ND) [@DAgostino2003-ot] and
Greenwood-Nam-D'Agostino (GND) [@Demler2015-yx] tests for survival outcomes.

## Implementation details

The statistical tests mentioned above, as well as the linear fit, rely on grouping instances.
First the instances in the dataset are ordered by their expected (predicted)
values. Within each group, the expected values and observed values are compared:

* Categorical outcomes: The fraction instances with the (positive) observed class in the group is
compared against the mean expected probability for that same class.

* Regression outcomes: Both expected and observed values are first normalised to
a $[0, 1]$ range using the value range observed in the development dataset. Then
the mean observed value in the group is compared against the mean expected
value.

* Survival outcomes: Survival probabilities are predicted at time points indicated by 
`evaluation_times`. The mean expected survival probability in the group is then
compared against the observed probability (Kaplan-Meier estimator) at the same
time points.

The points thus created can be observed in a calibration plot when `estimation_type="point"`:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_calibration_data(object=binomial_model,
                                         data=binomial_data,
                                         estimation_type="point")

grid::grid.newpage()
grid::grid.draw(plots[[1]])
```
The plot with point estimates and the earlier calibration plot with bootstrap
confidence intervals lead to the same assessment. However, you may have observed
that the number of points, and thus the number of groups, is certainly larger
than the 10 groups commonly formed for assessing calibration. This is correct.

Familiar introduces randomness into the assessment of calibration. First,
familiar randomises the number of groups drawn because using a fixed number of
10 groups is arbitrary. Second, the actual group sizes are randomised by drawing
group identifiers with replacement for all instances. The set of drawn
identifiers are then sorted, and the instances, ordered by increasing expected
value, are then assigned to their respective group. Thirdly, this process is
repeated multiple times, i.e. 20 times for point estimates, again 20 times (on
bootstraps) for bias-corrected estimates and more for bootstraps confidence
intervals.

Linear fit intercept and slope as well as the goodness-of-fit test p-value are
determined for each single set of groups. Intercept and slopes are then averaged (`point`
and `bias_correction`) or assessed as a distribution
(`bootstrap_confidence_interval`). p-values from goodness-of-fit tests need to
be combined. These tests are not fully independent because they derive
from the same dataset. We therefore compute a harmonic mean p-value
[@Wilson2019-bi] from the individual p-values.

# Decision curve analysis

Decision curve analysis is a method to assess the net (clinical) benefit of
models, introduced by Vickers and Elkin [@Vickers2006-be]. The decision curve
compares the model benefit for different threshold probabilities against two or
more options. The two standard options are that all instances receive an
intervention, and none receive an intervention. What constitutes an intervention
is model-dependent [@Vickers2019-ar]. For our breast cancer cohort, intervention
might constitute treatment of all patients, versus no treatment and treatment for
patients with malignancy predicted by the model. This produces the following
figure:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_decision_curve(object=binomial_model,
                                       data=binomial_data,
                                       y_range=c(-0.2, 0.5))

plots[[1]]
```
The shaded curve is the decision curve produced by the model, with 95% confidence
intervals. The declining curve represents the intervention for all, which offers
no benefit for a malignancy probability of 45%. The horizontal line at a net
benefit of 0.0 represent the no intervention option. As may be observed, the
model offers a net benefit over the entire range of threshold probabilities.

Decision curves are not defined for regression problems. Familiar can perform
decision curve analysis for survival outcomes, based on Vickers et al.
[@Vickers2008-uu].

# Variable importance

Not all features are related to the outcome. One of the most critical points in
machine learning is selecting those features that are important enough to
incorporate into a model. Variable importance is assessed prospectively during
feature selection step. For the purpose of explaining which features are
considered important for the model itself, familiar employs both model-specific
and model-agnostic methods.

## Model-specific methods

Model-specific methods rely on aspects of the models themselves to determine
variable importance. For instance, the coefficients of regression models are a
measure of variable importance when features are normalised. In the case of the
random forest model for the breast cancer dataset, permutation variable
importance is determined.This produces the following plot:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_model_signature_variable_importance(object=binomial_model,
                                                            data=binomial_data)

plots[[1]]
```
Features are always ordered with the most important features on the left, and
least important features on the right.

In case an ensemble of models is assessed, scores are aggregated using an
aggregation method (see the *feature selection methods* vignette). This method
can be set using the `aggregation_method` argument, or the
`eval_aggregation_method` configuration parameter (`summon_familiar`).

The `plot_model_signature_occurrence` method plots the occurrence of features
among the first 5 ranks across an ensemble of models (if available). The rank
threshold can be specified using the `rank_threshold` argument or the
`eval_aggregation_rank_threshold` parameter (`summon_familiar`).

## Permutation variable importance

Permutation variable importance is a model-agnostic variable importance method.
It assesses importance by measuring the decrease in model performance caused by
shuffling values of a feature across the instances in a dataset.

For the breast cancer dataset, this results in the following figure:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_permutation_variable_importance(object=binomial_model,
                                                        data=binomial_data,
                                                        estimation_type="bias_correction")

plots[[1]]
```
The figure shows the features along the y-axis, and the loss in AUC-ROC caused
by permuting the feature values along the x-axis. Accordingly,
`cell_shape_uniformity` is considered to be the most important feature in the
model.

The figure shows two bars for each feature. A well-known issue with permutation
variable importance is that the presence of highly correlated features causes
permutation variable importance to become unreliable. If two features are both
important and highly correlated, shuffling one of them does not decrease model
performance as much. For this reason, familiar will permute features together
depending on their mutual correlation, and the threshold values.

By default, similarity between features is assessed using McFadden's pseudo-R^2.
This metric can assess similarity between different types of features, i.e.
numeric and categorical. The default thresholds are 0.3 and 1.0 that
respectively cluster highly similar features and permute all features
individually.

The breast cancer dataset doesn't contain features that exceed the relatively
stringent threshold of 0.3. Since the breast cancer dataset only contains
numeric values, we can also measure similarity differently. For example, if we
measure similarity using the Spearman`s rank correlation coefficient, we arrive
at the following:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_permutation_variable_importance(object=binomial_model,
                                                        data=binomial_data,
                                                        estimation_type="bias_correction",
                                                        feature_similarity_metric="spearman",
                                                        feature_similarity_threshold=c(0.6, 1.0))

plots[[1]]
```
This figure paints a different picture: `cell_shape_uniformity`, `bare_nuclei`,
`normal_nucleoli` and `epithelial_cell_size` are moderately to strongly
correlated. When permuted together, this has a major effect on model
performance. This can also be observed in the performance decrease caused by
shuffling `cell_shape_uniformity` alone, which only partially matches the
performance decrease caused by shuffling the 4 features. The other 3 features
have little individual impact on performance decrease, but shuffling them
together directly shows that these features are important as well.

# Partial dependence and individual conditional expectation plots

Variable importance does not explain how features influence the model. Partial
dependence (PD) [@Friedman2001-jf] and individual conditional expectation (ICE)
[@Goldstein2015-rv] plots can be used to visualise this behaviour. As an
example, we can create an ICE plot for the `cell_shape_uniformity` feature that
was found to be important for predicting cell malignancy in the breast cancer.

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_ice(object=binomial_model,
                            data=binomial_data,
                            features="cell_shape_uniformity")

grid::grid.newpage()
grid::grid.draw(plots[[1]])
```
The ICE plot shows multiple curves for individual samples, as well as their
average, the partial dependence plot as a somewhat thicker curve. The ICE plot
indicates that an increase in cell shape uniformity is associated by the model
with a rise in malignancy.

It also shows novelty, which is a measure of how out-of-distribution the value
is. Novelty is computed using extended isolation forests [@Hariri2019], and is
the average normalised height at which a sample is found in a terminal leaf node
of the trees in the isolation forest [@Liu2008-kw]. Novelty values range between
`0.00` and `1.00`. Some care in interpretation is needed. While increasing
novelty values represents instances that lie further from the distribution, the
values that represent in-distribution instances may vary. In the ICE plot above,
novelty values range between `0.40` and `0.60`. In fact, the median novelty
value for the training instances are `0.47` with an interquartile range of
`0.07`. None of the points in ICE plot can therefore be considered
out-of-distribution.

It is possible to anchor the curves in the ICE plot to a specific feature value.
This allows for assessing the model response for individual instances versus a
fixed value. In effect this can reduce some of the offset caused by other
features for instance, and may help to elucidate the observed behaviour:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_ice(object=binomial_model,
                            data=binomial_data,
                            features="cell_shape_uniformity",
                            anchor_values=list("cell_shape_uniformity"=10.0))

grid::grid.newpage()
grid::grid.draw(plots[[1]])
```

In the plot above, we have anchored the values at `cell_shape_uniformity=10`,
which is the maximum value that this feature can take. All curves are then drawn
relative to the predicted probability found for this particular value. 

It is also possible to show the partial dependence of two features at the same
time:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_ice(object=binomial_model,
                            data=binomial_data,
                            features=c("cell_shape_uniformity", "bare_nuclei"))

grid::grid.newpage()
grid::grid.draw(plots[[1]])
```
The partial dependence plot above shows the `cell_shape_uniformity` along the
x-axis and `bare_nuclei` along the y-axis.  Higher intensities indicate higher
expected probability of malignancy. This indicates that higher values of
`cell_shape_uniformity` clearly affect the predicted malignancy probability.
Higher values of `bare_nuclei` also lead to higher malignancy probabilities, but
the effect is less pronounced. The size of the points is indicated by the
novelty scores.

It also possible to anchor 2D partial dependence plots. 

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_ice(object=binomial_model,
                            data=binomial_data,
                            features=c("cell_shape_uniformity", "bare_nuclei"),
                            anchor_values=list("cell_shape_uniformity"=10.0,
                                               "bare_nuclei"=10.0),
                            show_novelty=FALSE)

grid::grid.newpage()
grid::grid.draw(plots[[1]])
```
We disabled novelty (`show_novelty=FALSE`) for plotting the 2D anchored plot.
This changes the appearance of the plot, which now consists of coloured
rectangles. The white dots indicate the sampled values.

Familiar automatically determines the values at which numerical features are
sampled based on their distribution in the development dataset. Such values can
also be manually set using the `feature_x_range` and `feature_y_range`
arguments. Whereas for the breast cancer dataset the features are linearly
sampled, for the `nodes` feature in the colon cancer dataset this is not the
case:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_ice(object=survival_model,
                            data=survival_data,
                            features=c("nodes", "rx"),
                            evaluation_times=1000,
                            show_novelty=FALSE)

grid::grid.newpage()
grid::grid.draw(plots[[1]])
```
Here we observe that the number of nodes was not sampled linearly. Survival
probability at 1000 days drops with an increase in nodes. Moreover, the model
predicts that patients in the Levamisole `Lev` and observation `Obs` arms
have lower survival probabilities overall.

# Feature and sample similarity

Highly similar features can carry redundant information for a model. This can be
visualised using similarity heatmaps:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_feature_similarity(object=binomial_model,
                                           data=binomial_data,
                                           feature_similarity_metric="spearman")

grid::grid.newpage()
grid::grid.draw(plots[[1]])
```
The heatmap displays the spearman correlation coefficient between the different
features. The features are also ordered by similarity, so that clusters of
similar features can be identified. Note that the dendrograms depict cluster
distance, i.e. $1-|\rho|$ for the spearman correlation coefficient.

It is also possible to view the sample clustering. This can be used to identify
if samples are readily stratified into clusters, that may or may not correlate
to the endpoint of interest. For the breast cancer dataset, this yields the
following heatmap:
```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_sample_clustering(object=binomial_model,
                                          data=binomial_data,
                                          feature_similarity_metric="spearman")

grid::grid.newpage()
grid::grid.draw(plots[[1]])
```
As may be observed, samples in the breast cancer dataset form two main clusters
that correspond closely to cell malignancy. The top dendrogram is the same as
for feature clustering, as it should be. The right dendrogram is formed after
computing Gower's distance between samples (by default). The heatmap itself
shows normalised values for the features, based on normalisation and
transformation parameters obtained during model development.

Normalisation options can be altered by changing the `show_normalised_data`
argument. In the breast cancer dataset all features have the same range, so we
can also show the data directly:

```{r, fig.align='center', fig.width=7}
plots <- familiar::plot_sample_clustering(object=binomial_model,
                                          data=binomial_data,
                                          feature_similarity_metric="spearman",
                                          show_normalised_data="none")

grid::grid.newpage()
grid::grid.draw(plots[[1]])
```
